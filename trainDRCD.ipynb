{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trainDRCD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1Rru4MXtiSQ0"
      ],
      "authorship_tag": "ABX9TyNVM3uYt3pO9lYSGqAyojSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric88525/DRCD/blob/master/trainDRCD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cn-gQTGhC7z",
        "colab_type": "text"
      },
      "source": [
        "# DRCD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avAR5VmvfDDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "7e106c28-ff76-46de-c53c-53cee945423e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Aug  7 15:49:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rru4MXtiSQ0",
        "colab_type": "text"
      },
      "source": [
        "# 該來用TPU了吧\n",
        "+ [REF](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb#scrollTo=H9KYz-Vk4fMa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC0vjMNQiXNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxeJekGjicKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "both",
        "outputId": "9c2de2f9-1086-42de-bdd4-40999e825b5e"
      },
      "source": [
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5115  100  5115    0     0  57471      0 --:--:-- --:--:-- --:--:-- 57471\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200325 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (49.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Uninstalling torch-1.6.0+cu101:\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.6.0+cu101\n",
            "Uninstalling torchvision-0.7.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 83.4 MiB/ 83.4 MiB]                                                \n",
            "Operation completed over 1 objects/83.4 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][114.5 MiB/114.5 MiB]                                                \n",
            "Operation completed over 1 objects/114.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (1.18.5)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+d6149a7\n",
            "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+e788e5b\n",
            "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200325) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (381 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtoIU1Dhift5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports pytorch\n",
        "import torch\n",
        "\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v94_bCAbijGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "74eee875-db0d-4342-985d-2ccd23c442bd"
      },
      "source": [
        "# Creates a random tensor on xla:1 (a Cloud TPU core)\n",
        "device = xm.xla_device()\n",
        "t1 = torch.ones(3, 3, device = device)\n",
        "print(t1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], device='xla:1')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gdLbhwM65G3",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6uc8HZegupi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5e14a96f-bf7f-475a-f05a-e3e72ee47980"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7N2Uzjegwxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "084814f9-0b90-41bb-caff-af98ee218273"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install opencc\n",
        "!pip install pyprind\n",
        "!pip install zhon"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8c654093d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install opencc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyprind'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install zhon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uubPXZfugyLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a13176bc-25cd-4e8b-c079-ee9e7b8ad9d2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from transformers import *\n",
        "import pandas as pd\n",
        "import ast\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "from time import strftime,gmtime\n",
        "from opencc import OpenCC\n",
        "import pyprind\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "from zhon.hanzi import non_stops,stops\n",
        "import numpy as np\n",
        "import random\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RnZleZyg1jg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0853975-f053-44b6-9001-b6b49eaa8718"
      },
      "source": [
        "cd ./drive/My Drive/Colab Notebooks/DRCD"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DRCD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLaU5RUIqXsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdxaIBxO7uxY",
        "colab_type": "text"
      },
      "source": [
        "# Get Train Dev Test\n",
        "\n",
        "+ train 8014 26936\n",
        "+ test 1000 3493\n",
        "+ dev 1000 3524\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yry9u4sI-3zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "class DRCDdataset():\n",
        "  def __init__(self,train_path=None , test_path = None ,dev_path = None):\n",
        "    \n",
        "    if train_path != None:\n",
        "      self.train = self.get_data(train_path)\n",
        "    if test_path != None:\n",
        "      self.test = self.get_data(test_path)\n",
        "    if dev_path != None:\n",
        "      self.dev = self.get_data(dev_path)\n",
        "\n",
        "  def get_data(self,path):\n",
        "    input_file = open(path)\n",
        "    ds = json.load(input_file)\n",
        "    datas = []\n",
        "    for i in range(len(ds['data'])):\n",
        "      for j in ds['data'][i]['paragraphs']:\n",
        "        context = j['context']\n",
        "        question = [q['question'] for q in j['qas']]\n",
        "        ans = [ a['answers'][0]['text'] for a in j['qas'] ]\n",
        "        #ans = [a['answers'][0]['answer_start'] for a in j['qas']]\n",
        "        #anse = [a['answers'][0]['answer_start']+len(a['answers'][0]['text'])-1 for a in j['qas']]\n",
        "        for k in range(len(question)):\n",
        "          datas.append([context,question[k],ans[k]])\n",
        "          #datas.append([context,question[k],ans[k],anse[k]])\n",
        "    return datas\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P7DpSVMcy7Y",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFAFURdNcP77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "    def __init__(self,data,model_type,device,language):\n",
        "      self.data = data\n",
        "      self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "      self.device = device\n",
        "      self.cc = OpenCC('t2s') # tw->china\n",
        "      self.language = language\n",
        "    def __getitem__(self,idx):\n",
        "      paragraph,question,ans = self.data[idx][0] , self.data[idx][1], self.data[idx][2] \n",
        "      if self.language == 'china':\n",
        "        paragraph,question,ans = self.cc.convert(paragraph),self.cc.convert(question),self.cc.convert(ans)\n",
        "        \n",
        "      token_tensor = self.tokenizer.encode_plus(question,paragraph,max_length=512,truncation=True,pad_to_max_length=True)\n",
        "      #print(paragraph[:20],question,ans)\n",
        "      s_idx = [0]*512\n",
        "      e_idx = [0]*512\n",
        "      # 答案tok\n",
        "      s_tok = self.tokenizer.encode(ans)[1:-1]\n",
        "      # 找到開頭跟他一樣的\n",
        "      s_lsit = [i for i, x in enumerate(token_tensor['input_ids']) if x == s_tok[0]]\n",
        "      \n",
        "      for s_pos in s_lsit:\n",
        "        e_pos = s_pos+len(s_tok)\n",
        "        if e_pos > 511:\n",
        "          continue\n",
        "          \n",
        "        if token_tensor['input_ids'][s_pos:e_pos] == s_tok:\n",
        "          s_idx[s_pos] = 1\n",
        "          e_idx[e_pos-1] = 1\n",
        "          break\n",
        "      s_tensor = torch.Tensor(s_idx)\n",
        "      e_tensor = torch.Tensor(e_idx)\n",
        "      # input_ids / token_type_ids / attention_mask / s_tensor / e_tensor\n",
        "      return {'input_ids': torch.tensor(token_tensor['input_ids']).to(self.device)\n",
        "          ,'token_type_ids': torch.tensor( token_tensor['token_type_ids']).to(self.device) \n",
        "          ,'attention_mask': torch.tensor( token_tensor['attention_mask']).to(self.device)\n",
        "          ,'s_tensor': s_tensor.to(self.device)\n",
        "          ,'e_tensor': e_tensor.to(self.device)}\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84vSbr-Ynxkp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46e83f97-bc8b-402a-d080-cec0da695ceb"
      },
      "source": [
        "\n",
        "x = DRCDdataset(train_path='./dataset/DRCD_train.json',test_path='./dataset/DRCD_test.json',dev_path='./dataset/DRCD_dev.json')\n",
        "print(len(x.train),len(x.test),len(x.dev))\n",
        "device = torch.device('cuda:0')\n",
        "model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "language = 'china'\n",
        "z = GetDataset(x.dev,model_type,device,language)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "i = 10\n",
        "\n",
        "tok = tokenizer.convert_ids_to_tokens(z[i]['input_ids'])\n",
        "s = z[i]['s_tensor'].tolist()\n",
        "e = z[i]['e_tensor'].tolist()\n",
        "tp = z[i]['token_type_ids'].tolist()\n",
        "at = z[i]['attention_mask'].tolist()\n",
        "for i in range(len(tok)):\n",
        "  print(tok[i],s[i],e[i],tp[i],at[i])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26936 3493 3524\n",
            "[CLS] 0.0 0.0 0 1\n",
            "负 0.0 0.0 0 1\n",
            "责 0.0 0.0 0 1\n",
            "管 0.0 0.0 0 1\n",
            "理 0.0 0.0 0 1\n",
            "马 0.0 0.0 0 1\n",
            "祖 0.0 0.0 0 1\n",
            "国 0.0 0.0 0 1\n",
            "家 0.0 0.0 0 1\n",
            "风 0.0 0.0 0 1\n",
            "景 0.0 0.0 0 1\n",
            "区 0.0 0.0 0 1\n",
            "的 0.0 0.0 0 1\n",
            "单 0.0 0.0 0 1\n",
            "位 0.0 0.0 0 1\n",
            "为 0.0 0.0 0 1\n",
            "？ 0.0 0.0 0 1\n",
            "[SEP] 0.0 0.0 0 1\n",
            "马 0.0 0.0 1 1\n",
            "祖 0.0 0.0 1 1\n",
            "列 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "是 0.0 0.0 1 1\n",
            "隶 0.0 0.0 1 1\n",
            "属 0.0 0.0 1 1\n",
            "中 0.0 0.0 1 1\n",
            "华 0.0 0.0 1 1\n",
            "民 0.0 0.0 1 1\n",
            "国 0.0 0.0 1 1\n",
            "的 0.0 0.0 1 1\n",
            "群 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "位 0.0 0.0 1 1\n",
            "于 0.0 0.0 1 1\n",
            "台 0.0 0.0 1 1\n",
            "湾 0.0 0.0 1 1\n",
            "海 0.0 0.0 1 1\n",
            "峡 0.0 0.0 1 1\n",
            "正 0.0 0.0 1 1\n",
            "北 0.0 0.0 1 1\n",
            "方 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "面 0.0 0.0 1 1\n",
            "临 0.0 0.0 1 1\n",
            "闽 0.0 0.0 1 1\n",
            "江 0.0 0.0 1 1\n",
            "口 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "连 0.0 0.0 1 1\n",
            "江 0.0 0.0 1 1\n",
            "口 0.0 0.0 1 1\n",
            "和 0.0 0.0 1 1\n",
            "罗 0.0 0.0 1 1\n",
            "源 0.0 0.0 1 1\n",
            "湾 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "与 0.0 0.0 1 1\n",
            "中 0.0 0.0 1 1\n",
            "国 0.0 0.0 1 1\n",
            "大 0.0 0.0 1 1\n",
            "陆 0.0 0.0 1 1\n",
            "仅 0.0 0.0 1 1\n",
            "一 0.0 0.0 1 1\n",
            "水 0.0 0.0 1 1\n",
            "之 0.0 0.0 1 1\n",
            "隔 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "距 0.0 0.0 1 1\n",
            "中 0.0 0.0 1 1\n",
            "国 0.0 0.0 1 1\n",
            "大 0.0 0.0 1 1\n",
            "陆 0.0 0.0 1 1\n",
            "最 0.0 0.0 1 1\n",
            "近 0.0 0.0 1 1\n",
            "点 0.0 0.0 1 1\n",
            "约 0.0 0.0 1 1\n",
            "9 0.0 0.0 1 1\n",
            ". 0.0 0.0 1 1\n",
            "25 0.0 0.0 1 1\n",
            "公 0.0 0.0 1 1\n",
            "里 0.0 0.0 1 1\n",
            "。 0.0 0.0 1 1\n",
            "主 0.0 0.0 1 1\n",
            "要 0.0 0.0 1 1\n",
            "由 0.0 0.0 1 1\n",
            "南 0.0 0.0 1 1\n",
            "竿 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "北 0.0 0.0 1 1\n",
            "竿 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "高 0.0 0.0 1 1\n",
            "登 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "亮 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "东 0.0 0.0 1 1\n",
            "莒 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "西 0.0 0.0 1 1\n",
            "莒 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "东 0.0 0.0 1 1\n",
            "引 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "西 0.0 0.0 1 1\n",
            "引 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "及 0.0 0.0 1 1\n",
            "其 0.0 0.0 1 1\n",
            "附 0.0 0.0 1 1\n",
            "属 0.0 0.0 1 1\n",
            "小 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "共 0.0 0.0 1 1\n",
            "计 0.0 0.0 1 1\n",
            "36 0.0 0.0 1 1\n",
            "个 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "屿 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "礁 0.0 0.0 1 1\n",
            "屿 0.0 0.0 1 1\n",
            "组 0.0 0.0 1 1\n",
            "成 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "面 0.0 0.0 1 1\n",
            "积 0.0 0.0 1 1\n",
            "29 0.0 0.0 1 1\n",
            ". 0.0 0.0 1 1\n",
            "6 0.0 0.0 1 1\n",
            "平 0.0 0.0 1 1\n",
            "方 0.0 0.0 1 1\n",
            "公 0.0 0.0 1 1\n",
            "里 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "居 0.0 0.0 1 1\n",
            "民 0.0 0.0 1 1\n",
            "人 0.0 0.0 1 1\n",
            "口 0.0 0.0 1 1\n",
            "约 0.0 0.0 1 1\n",
            "12 0.0 0.0 1 1\n",
            ", 0.0 0.0 1 1\n",
            "600 0.0 0.0 1 1\n",
            "多 0.0 0.0 1 1\n",
            "人 0.0 0.0 1 1\n",
            "。 0.0 0.0 1 1\n",
            "1999 0.0 0.0 1 1\n",
            "年 0.0 0.0 1 1\n",
            "以 0.0 0.0 1 1\n",
            "马 0.0 0.0 1 1\n",
            "祖 0.0 0.0 1 1\n",
            "列 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "为 0.0 0.0 1 1\n",
            "范 0.0 0.0 1 1\n",
            "围 0.0 0.0 1 1\n",
            "的 0.0 0.0 1 1\n",
            "马 0.0 0.0 1 1\n",
            "祖 0.0 0.0 1 1\n",
            "国 0.0 0.0 1 1\n",
            "家 0.0 0.0 1 1\n",
            "风 0.0 0.0 1 1\n",
            "景 0.0 0.0 1 1\n",
            "区 0.0 0.0 1 1\n",
            "成 0.0 0.0 1 1\n",
            "立 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "隶 0.0 0.0 1 1\n",
            "属 0.0 0.0 1 1\n",
            "于 0.0 0.0 1 1\n",
            "交 1.0 0.0 1 1\n",
            "通 0.0 0.0 1 1\n",
            "部 0.0 0.0 1 1\n",
            "观 0.0 0.0 1 1\n",
            "光 0.0 0.0 1 1\n",
            "局 0.0 1.0 1 1\n",
            "管 0.0 0.0 1 1\n",
            "理 0.0 0.0 1 1\n",
            "。 0.0 0.0 1 1\n",
            "除 0.0 0.0 1 1\n",
            "了 0.0 0.0 1 1\n",
            "地 0.0 0.0 1 1\n",
            "形 0.0 0.0 1 1\n",
            "地 0.0 0.0 1 1\n",
            "貌 0.0 0.0 1 1\n",
            "和 0.0 0.0 1 1\n",
            "人 0.0 0.0 1 1\n",
            "文 0.0 0.0 1 1\n",
            "特 0.0 0.0 1 1\n",
            "色 0.0 0.0 1 1\n",
            "外 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "因 0.0 0.0 1 1\n",
            "地 0.0 0.0 1 1\n",
            "理 0.0 0.0 1 1\n",
            "位 0.0 0.0 1 1\n",
            "置 0.0 0.0 1 1\n",
            "关 0.0 0.0 1 1\n",
            "系 0.0 0.0 1 1\n",
            "而 0.0 0.0 1 1\n",
            "成 0.0 0.0 1 1\n",
            "为 0.0 0.0 1 1\n",
            "多 0.0 0.0 1 1\n",
            "种 0.0 0.0 1 1\n",
            "候 0.0 0.0 1 1\n",
            "鸟 0.0 0.0 1 1\n",
            "过 0.0 0.0 1 1\n",
            "境 0.0 0.0 1 1\n",
            "或 0.0 0.0 1 1\n",
            "渡 0.0 0.0 1 1\n",
            "冬 0.0 0.0 1 1\n",
            "的 0.0 0.0 1 1\n",
            "区 0.0 0.0 1 1\n",
            "域 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "2000 0.0 0.0 1 1\n",
            "年 0.0 0.0 1 1\n",
            "成 0.0 0.0 1 1\n",
            "立 0.0 0.0 1 1\n",
            "马 0.0 0.0 1 1\n",
            "祖 0.0 0.0 1 1\n",
            "列 0.0 0.0 1 1\n",
            "岛 0.0 0.0 1 1\n",
            "燕 0.0 0.0 1 1\n",
            "鸥 0.0 0.0 1 1\n",
            "保 0.0 0.0 1 1\n",
            "护 0.0 0.0 1 1\n",
            "区 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "主 0.0 0.0 1 1\n",
            "要 0.0 0.0 1 1\n",
            "针 0.0 0.0 1 1\n",
            "对 0.0 0.0 1 1\n",
            "白 0.0 0.0 1 1\n",
            "眉 0.0 0.0 1 1\n",
            "燕 0.0 0.0 1 1\n",
            "鸥 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "红 0.0 0.0 1 1\n",
            "燕 0.0 0.0 1 1\n",
            "鸥 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "苍 0.0 0.0 1 1\n",
            "燕 0.0 0.0 1 1\n",
            "鸥 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "凤 0.0 0.0 1 1\n",
            "头 0.0 0.0 1 1\n",
            "燕 0.0 0.0 1 1\n",
            "鸥 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "黑 0.0 0.0 1 1\n",
            "尾 0.0 0.0 1 1\n",
            "鸥 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "岩 0.0 0.0 1 1\n",
            "鹭 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "插 0.0 0.0 1 1\n",
            "尾 0.0 0.0 1 1\n",
            "雨 0.0 0.0 1 1\n",
            "燕 0.0 0.0 1 1\n",
            "等 0.0 0.0 1 1\n",
            "7 0.0 0.0 1 1\n",
            "种 0.0 0.0 1 1\n",
            "鸟 0.0 0.0 1 1\n",
            "类 0.0 0.0 1 1\n",
            "。 0.0 0.0 1 1\n",
            "2000 0.0 0.0 1 1\n",
            "年 0.0 0.0 1 1\n",
            "6 0.0 0.0 1 1\n",
            "月 0.0 0.0 1 1\n",
            "曾 0.0 0.0 1 1\n",
            "在 0.0 0.0 1 1\n",
            "此 0.0 0.0 1 1\n",
            "发 0.0 0.0 1 1\n",
            "现 0.0 0.0 1 1\n",
            "在 0.0 0.0 1 1\n",
            "世 0.0 0.0 1 1\n",
            "界 0.0 0.0 1 1\n",
            "鸟 0.0 0.0 1 1\n",
            "类 0.0 0.0 1 1\n",
            "红 0.0 0.0 1 1\n",
            "皮 0.0 0.0 1 1\n",
            "书 0.0 0.0 1 1\n",
            "中 0.0 0.0 1 1\n",
            "被 0.0 0.0 1 1\n",
            "列 0.0 0.0 1 1\n",
            "为 0.0 0.0 1 1\n",
            "临 0.0 0.0 1 1\n",
            "绝 0.0 0.0 1 1\n",
            "种 0.0 0.0 1 1\n",
            "的 0.0 0.0 1 1\n",
            "黑 0.0 0.0 1 1\n",
            "嘴 0.0 0.0 1 1\n",
            "端 0.0 0.0 1 1\n",
            "凤 0.0 0.0 1 1\n",
            "头 0.0 0.0 1 1\n",
            "燕 0.0 0.0 1 1\n",
            "鸥 0.0 0.0 1 1\n",
            "共 0.0 0.0 1 1\n",
            "4 0.0 0.0 1 1\n",
            "对 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "每 0.0 0.0 1 1\n",
            "年 0.0 0.0 1 1\n",
            "的 0.0 0.0 1 1\n",
            "最 0.0 0.0 1 1\n",
            "佳 0.0 0.0 1 1\n",
            "赏 0.0 0.0 1 1\n",
            "燕 0.0 0.0 1 1\n",
            "鸥 0.0 0.0 1 1\n",
            "季 0.0 0.0 1 1\n",
            "为 0.0 0.0 1 1\n",
            "5 0.0 0.0 1 1\n",
            "月 0.0 0.0 1 1\n",
            "到 0.0 0.0 1 1\n",
            "8 0.0 0.0 1 1\n",
            "月 0.0 0.0 1 1\n",
            "。 0.0 0.0 1 1\n",
            "马 0.0 0.0 1 1\n",
            "祖 0.0 0.0 1 1\n",
            "除 0.0 0.0 1 1\n",
            "通 0.0 0.0 1 1\n",
            "用 0.0 0.0 1 1\n",
            "国 0.0 0.0 1 1\n",
            "语 0.0 0.0 1 1\n",
            "之 0.0 0.0 1 1\n",
            "外 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "还 0.0 0.0 1 1\n",
            "通 0.0 0.0 1 1\n",
            "用 0.0 0.0 1 1\n",
            "操 0.0 0.0 1 1\n",
            "长 0.0 0.0 1 1\n",
            "乐 0.0 0.0 1 1\n",
            "口 0.0 0.0 1 1\n",
            "音 0.0 0.0 1 1\n",
            "的 0.0 0.0 1 1\n",
            "闽 0.0 0.0 1 1\n",
            "东 0.0 0.0 1 1\n",
            "语 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "当 0.0 0.0 1 1\n",
            "地 0.0 0.0 1 1\n",
            "称 0.0 0.0 1 1\n",
            "为 0.0 0.0 1 1\n",
            "「 0.0 0.0 1 1\n",
            "平 0.0 0.0 1 1\n",
            "话 0.0 0.0 1 1\n",
            "」 0.0 0.0 1 1\n",
            "、 0.0 0.0 1 1\n",
            "「 0.0 0.0 1 1\n",
            "马 0.0 0.0 1 1\n",
            "祖 0.0 0.0 1 1\n",
            "话 0.0 0.0 1 1\n",
            "」 0.0 0.0 1 1\n",
            "或 0.0 0.0 1 1\n",
            "「 0.0 0.0 1 1\n",
            "福 0.0 0.0 1 1\n",
            "州 0.0 0.0 1 1\n",
            "话 0.0 0.0 1 1\n",
            "」 0.0 0.0 1 1\n",
            "， 0.0 0.0 1 1\n",
            "官 0.0 0.0 1 1\n",
            "方 0.0 0.0 1 1\n",
            "则 0.0 0.0 1 1\n",
            "称 0.0 0.0 1 1\n",
            "之 0.0 0.0 1 1\n",
            "为 0.0 0.0 1 1\n",
            "「 0.0 0.0 1 1\n",
            "福 0.0 0.0 1 1\n",
            "州 0.0 0.0 1 1\n",
            "语 0.0 0.0 1 1\n",
            "」 0.0 0.0 1 1\n",
            "。 0.0 0.0 1 1\n",
            "[SEP] 0.0 0.0 1 1\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n",
            "[PAD] 0.0 0.0 0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z-zDS64xoOO",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AHDfYFexq-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bertDRCD(nn.Module):\n",
        "    def __init__(self,model_type):\n",
        "        super(bertDRCD,self).__init__()\n",
        "\n",
        "        config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "        self.bert_model = BertModel.from_pretrained(model_type,config = config)\n",
        "  \n",
        "        self.s_vector = nn.Parameter(torch.randn(config.hidden_size),requires_grad=True) \n",
        "        self.e_vector = nn.Parameter(torch.randn(config.hidden_size),requires_grad=True) \n",
        "\n",
        "        #self.start()\n",
        "\n",
        "    def start(self):\n",
        "      self.decoder = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size,config.hidden_size)\n",
        "            ,nn.Dropout(0.1)\n",
        "            ,nn.ReLU()\n",
        "            ,nn.Linear(config.hidden_size,1)\n",
        "      )     \n",
        "      nn.init.xavier_uniform_(self.s_decoder[0].weight)\n",
        "      nn.init.constant_(self.s_decoder[0].bias, 0)\n",
        "      nn.init.xavier_uniform_(self.s_decoder[3].weight)\n",
        "      nn.init.constant_(self.s_decoder[3].bias, 0)\n",
        "      nn.init.xavier_uniform_(self.e_decoder[0].weight)\n",
        "      nn.init.constant_(self.e_decoder[0].bias, 0)\n",
        "      nn.init.xavier_uniform_(self.e_decoder[3].weight)\n",
        "      nn.init.constant_(self.e_decoder[3].bias, 0)\n",
        "\n",
        "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None): \n",
        "      # hidden (batch,seq_len,hidden)\n",
        "      hidden = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[0]\n",
        "      s = torch.sum(hidden*self.s_vector,-1) # s now is (batch,seq_len)\n",
        "      M = token_type_ids.clone().float().to(hidden.device).detach()\n",
        "      M[M != 1] = float('-inf')\n",
        "      s = s+M \n",
        "      #s = torch.softmax(s,dim=-1)\n",
        "      e = torch.sum(hidden*self.e_vector,-1)\n",
        "      e = e + M\n",
        "      #e = torch.softmax(e,dim=-1)\n",
        "      return s,e"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGgoN41Cwtue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "device = torch.device('cpu')\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "model_type = 'bert-base-chinese'\n",
        "model = bertDRCD(model_type).to(device)\n",
        "ds = DRCDdataset(train_path='./dataset/DRCD_train.json',test_path='./dataset/DRCD_test.json',dev_path='./dataset/DRCD_dev.json')\n",
        "dataset = GetDataset(ds.dev,model_type,device)\n",
        "trainLoader = DataLoader(dataset, batch_size=8  ,shuffle=True)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for i , batch in enumerate(trainLoader):\n",
        "  #print(batch)\n",
        "  inp_ids,tok_id,att_m ,slabel,elabel = batch['input_ids'],batch['token_type_ids'],batch['attention_mask'],batch['s_tensor'],batch['e_tensor']\n",
        "  s,e = model(input_ids=inp_ids,attention_mask=att_m,token_type_ids=tok_id)\n",
        "  print(s)\n",
        "  print(e)\n",
        "  loss = (criterion(s,slabel) + criterion(e,elabel)) / 2\n",
        "  print(loss)\n",
        "  \n",
        "  break\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8iCJVtzUVBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tNzN6c-xZ_A",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YduImGhxb6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,ds,args,tp):\n",
        "  if tp == 'test':\n",
        "    print(f'Testing...{len(dataset)}')\n",
        "    dataset = GetDataset(ds.test,args.model_type,args.device,args.language)\n",
        "    \n",
        "  elif tp == 'dev':   \n",
        "    dataset = GetDataset(ds.dev,args.model_type,args.device,args.language)\n",
        "    print(f'Dev...{len(dataset)}')\n",
        "  loader = DataLoader(dataset,batch_size=args.batch_size,shuffle=True)\n",
        "  model.eval()\n",
        "  loss,i = 0,0\n",
        "  criterion = nn.BCELoss()\n",
        "  with torch.no_grad():\n",
        "    for batch in loader:\n",
        "      i = i+1\n",
        "      inp_ids,tok_id,att_m ,slabel,elabel = batch['input_ids'],batch['token_type_ids'],batch['attention_mask'],batch['s_tensor'],batch['e_tensor']\n",
        "      s,e = model(input_ids=inp_ids,attention_mask=att_m,token_type_ids=tok_id)\n",
        "      batch_loss = (criterion(s,slabel) + criterion(e,elabel)) / 2\n",
        "      loss += batch_loss\n",
        "  print(f'Result: LOSS: {loss} AVG: {loss/i} ')\n",
        "  return loss/i"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYP5p7oaxXQF",
        "colab_type": "text"
      },
      "source": [
        "# Train\n",
        "+ train(ds,w_d,lr_rate,device,model_type,epoches):\n",
        "+ data(還沒封裝) , weight_d , learningrate , device , model_type , epoches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGbz1SUhxcb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(ds,args):\n",
        "  dataset = GetDataset(ds.train,args.model_type,args.device,args.language)\n",
        "  trainLoader = DataLoader(dataset,batch_size=args.batch_size,shuffle=True)\n",
        "  model = bertDRCD(args.model_type).to(args.device)\n",
        "  parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  optimizer = AdamW(parameters, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "  criterion = nn.BCELoss()\n",
        "  model.train()\n",
        "  minloss = 100000\n",
        "  for ei in range(args.epoch):\n",
        "    loss,i,check_loss = 0,0,0\n",
        "    for batch in trainLoader:\n",
        "      i+=1\n",
        "      inp_ids,tok_id,att_m ,slabel,elabel = batch['input_ids'],batch['token_type_ids'],batch['attention_mask'],batch['s_tensor'],batch['e_tensor']\n",
        "      s,e = model(input_ids=inp_ids,attention_mask=att_m,token_type_ids=tok_id)\n",
        "      #print(f's {s} \\n e {e} \\n sl {torch.softmax(s,dim=-1)} \\n el {torch.softmax(e,dim=-1)} \\n ')\n",
        "      #print(f'SL {slabel} \\n EL {elabel}')\n",
        "      batch_loss = (criterion(torch.softmax(s,dim=-1),slabel) + criterion(torch.softmax(e,dim=-1),elabel)) / 2\n",
        "      loss += batch_loss\n",
        "      check_loss += batch_loss\n",
        "      #print(batch_loss)\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      if i % 500==0:\n",
        "        print(f'500 batch AVGLOSS {check_loss/1000}')\n",
        "        check_loss = 0\n",
        "    print(f'Epoches: {ei} Loss {loss} AVG: {loss/i}')\n",
        "\n",
        "    if loss < minloss:\n",
        "      minloss = loss\n",
        "      best_model = copy.deepcopy(model.state_dict())\n",
        "  return best_model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llu-ld14gYdA",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8P9_YXsgaCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0d26f7b-3f4a-4300-f0a8-0feb6d9e9874"
      },
      "source": [
        "parser = argparse.ArgumentParser([])\n",
        "parser.add_argument('--batch-size', default=8 , type=int)\n",
        "parser.add_argument('--epoch', default=5, type=int)\n",
        "parser.add_argument('--learning-rate', default=1e-5, type=float)    \n",
        "parser.add_argument('--weight-decay', default=0.001, type=float)\n",
        "parser.add_argument('--model-type', default='hfl/chinese-roberta-wwm-ext' , type=str)  #model_type = 'hfl/chinese-bert-wwm'  'hfl/chinese-roberta-wwm-ext'  'hfl/chinese-roberta-wwm-ext-large'\n",
        "parser.add_argument('--device', default=torch.device('cuda:0'), type=int)\n",
        "parser.add_argument('--language', default='china', type=str)\n",
        "args = parser.parse_args([])\n",
        "args"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(batch_size=8, device=device(type='cuda', index=0), epoch=5, language='china', learning_rate=1e-05, model_type='hfl/chinese-roberta-wwm-ext', weight_decay=0.001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac4TkXWchw4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = DRCDdataset(train_path='./dataset/DRCD_train.json',test_path='./dataset/DRCD_test.json',dev_path='./dataset/DRCD_dev.json')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHUEmYLLq1C7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "50541229-7afa-4efe-ea24-a2a3a7330e12"
      },
      "source": [
        "ds.train[:5]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['2010年引進的廣州快速公交運輸系統，屬世界第二大快速公交系統，日常載客量可達100萬人次，高峰時期每小時單向客流高達26900人次，僅次於波哥大的快速交通系統，平均每10秒鐘就有一輛巴士，每輛巴士單向行駛350小時。包括橋樑在內的站台是世界最長的州快速公交運輸系統站台，長達260米。目前廣州市區的計程車和公共汽車主要使用液化石油氣作燃料，部分公共汽車更使用油電、氣電混合動力技術。2012年底開始投放液化天然氣燃料的公共汽車，2014年6月開始投放液化天然氣插電式混合動力公共汽車，以取代液化石油氣公共汽車。2007年1月16日，廣州市政府全面禁止在市區內駕駛摩托車。違反禁令的機動車將會予以沒收。廣州市交通局聲稱禁令的施行，使得交通擁擠問題和車禍大幅減少。廣州白雲國際機場位於白雲區與花都區交界，2004年8月5日正式投入運營，屬中國交通情況第二繁忙的機場。該機場取代了原先位於市中心的無法滿足日益增長航空需求的舊機場。目前機場有三條飛機跑道，成為國內第三個擁有三跑道的民航機場。比鄰近的香港國際機場第三跑道預計的2023年落成早8年。',\n",
              "  '廣州的快速公交運輸系統每多久就會有一輛巴士？',\n",
              "  '10秒鐘'],\n",
              " ['2010年引進的廣州快速公交運輸系統，屬世界第二大快速公交系統，日常載客量可達100萬人次，高峰時期每小時單向客流高達26900人次，僅次於波哥大的快速交通系統，平均每10秒鐘就有一輛巴士，每輛巴士單向行駛350小時。包括橋樑在內的站台是世界最長的州快速公交運輸系統站台，長達260米。目前廣州市區的計程車和公共汽車主要使用液化石油氣作燃料，部分公共汽車更使用油電、氣電混合動力技術。2012年底開始投放液化天然氣燃料的公共汽車，2014年6月開始投放液化天然氣插電式混合動力公共汽車，以取代液化石油氣公共汽車。2007年1月16日，廣州市政府全面禁止在市區內駕駛摩托車。違反禁令的機動車將會予以沒收。廣州市交通局聲稱禁令的施行，使得交通擁擠問題和車禍大幅減少。廣州白雲國際機場位於白雲區與花都區交界，2004年8月5日正式投入運營，屬中國交通情況第二繁忙的機場。該機場取代了原先位於市中心的無法滿足日益增長航空需求的舊機場。目前機場有三條飛機跑道，成為國內第三個擁有三跑道的民航機場。比鄰近的香港國際機場第三跑道預計的2023年落成早8年。',\n",
              "  '從哪一天開始在廣州市內騎摩托車會被沒收？',\n",
              "  '2007年1月16日'],\n",
              " ['2010年引進的廣州快速公交運輸系統，屬世界第二大快速公交系統，日常載客量可達100萬人次，高峰時期每小時單向客流高達26900人次，僅次於波哥大的快速交通系統，平均每10秒鐘就有一輛巴士，每輛巴士單向行駛350小時。包括橋樑在內的站台是世界最長的州快速公交運輸系統站台，長達260米。目前廣州市區的計程車和公共汽車主要使用液化石油氣作燃料，部分公共汽車更使用油電、氣電混合動力技術。2012年底開始投放液化天然氣燃料的公共汽車，2014年6月開始投放液化天然氣插電式混合動力公共汽車，以取代液化石油氣公共汽車。2007年1月16日，廣州市政府全面禁止在市區內駕駛摩托車。違反禁令的機動車將會予以沒收。廣州市交通局聲稱禁令的施行，使得交通擁擠問題和車禍大幅減少。廣州白雲國際機場位於白雲區與花都區交界，2004年8月5日正式投入運營，屬中國交通情況第二繁忙的機場。該機場取代了原先位於市中心的無法滿足日益增長航空需求的舊機場。目前機場有三條飛機跑道，成為國內第三個擁有三跑道的民航機場。比鄰近的香港國際機場第三跑道預計的2023年落成早8年。',\n",
              "  '廣州白雲國際機場在完成第三條跑道的後八年哪一座機場也會有第三跑道？',\n",
              "  '香港國際機場'],\n",
              " ['廣州是京廣鐵路、廣深鐵路、廣茂鐵路、廣梅汕鐵路的終點站。2009年末，武廣客運專線投入運營，多單元列車覆蓋980公里的路程，最高時速可達350公里/小時。2011年1月7日，廣珠城際鐵路投入運營，平均時速可達200公里/小時。廣州鐵路、長途汽車和渡輪直達香港，廣九直通車從廣州東站開出，直達香港九龍紅磡站，總長度約182公里，車程在兩小時內。繁忙的長途汽車每年會從城市中的不同載客點把旅客接載至香港。在珠江靠市中心的北航道有渡輪線路，用於近江居民直接渡江而無需乘坐公交或步行過橋。南沙碼頭和蓮花山碼頭間每天都有高速雙體船往返，渡輪也開往香港中國客運碼頭和港澳碼頭。',\n",
              "  '廣珠城際鐵路平均每小時可以走多遠？',\n",
              "  '200公里'],\n",
              " ['廣州是京廣鐵路、廣深鐵路、廣茂鐵路、廣梅汕鐵路的終點站。2009年末，武廣客運專線投入運營，多單元列車覆蓋980公里的路程，最高時速可達350公里/小時。2011年1月7日，廣珠城際鐵路投入運營，平均時速可達200公里/小時。廣州鐵路、長途汽車和渡輪直達香港，廣九直通車從廣州東站開出，直達香港九龍紅磡站，總長度約182公里，車程在兩小時內。繁忙的長途汽車每年會從城市中的不同載客點把旅客接載至香港。在珠江靠市中心的北航道有渡輪線路，用於近江居民直接渡江而無需乘坐公交或步行過橋。南沙碼頭和蓮花山碼頭間每天都有高速雙體船往返，渡輪也開往香港中國客運碼頭和港澳碼頭。',\n",
              "  '廣九直通車從頭坐到尾約需要多久？',\n",
              "  '兩小時']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEyDq_PC1i82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "6e6dafc9-e7b0-408f-8b80-337644122cc1"
      },
      "source": [
        "mode = 'train'\n",
        "\n",
        "if mode == 'train': \n",
        "  print('Train')\n",
        "  best_model = train(ds,args)\n",
        "  if not os.path.exists('saved_models'):\n",
        "    os.makedirs('saved_models')    \n",
        "  modelname = 'bertDRCD'+'_'+(datetime.now()+timedelta(hours=8)).strftime(\"%m%d_%H%M\")+'.pt' \n",
        "  torch.save(best_model, f'saved_models/{modelname}')\n",
        "  print(f'Train end, model name is {modelname}.pt')\n",
        "\n",
        "elif mode == 'test' or mode == 'dev':\n",
        "  modelname = 'bertDRCD_0807_2222.pt'\n",
        "  test_model = bertDRCD(args.model_type).to(args.device)\n",
        "  test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
        "  test(test_model,ds,args,mode)\n",
        "\n",
        "#22.53021812438965\n",
        "#22.579822540283203"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "500 batch AVGLOSS 0.06845083832740784\n",
            "500 batch AVGLOSS 0.07170592993497849\n",
            "500 batch AVGLOSS 0.07431110739707947\n",
            "500 batch AVGLOSS 0.06905978918075562\n",
            "500 batch AVGLOSS 0.06781522184610367\n",
            "500 batch AVGLOSS 0.06675072759389877\n",
            "Epoches: 0 Loss 466.5035095214844 AVG: 0.13855168223381042\n",
            "500 batch AVGLOSS 0.06460896879434586\n",
            "500 batch AVGLOSS 0.06246434897184372\n",
            "500 batch AVGLOSS 0.06039474904537201\n",
            "500 batch AVGLOSS 0.05853394418954849\n",
            "500 batch AVGLOSS 0.057144973427057266\n",
            "500 batch AVGLOSS 0.056012626737356186\n",
            "Epoches: 1 Loss 399.83624267578125 AVG: 0.11875148862600327\n",
            "500 batch AVGLOSS 0.054885122925043106\n",
            "500 batch AVGLOSS 0.054207347333431244\n",
            "500 batch AVGLOSS 0.05415158346295357\n",
            "500 batch AVGLOSS 0.053444404155015945\n",
            "500 batch AVGLOSS 0.05336707457900047\n",
            "500 batch AVGLOSS 0.05288129672408104\n",
            "Epoches: 2 Loss 361.75439453125 AVG: 0.10744116455316544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7n5HS36ex16",
        "colab_type": "text"
      },
      "source": [
        "# Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVfxojHYfKJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DRCD():\n",
        "  def __init__(self,model_path,model_type): \n",
        "    \n",
        "    # device\n",
        "    self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    \n",
        "    # model & tokenizer\n",
        "   # model_type = 'hfl/chinese-roberta-wwm-ext'\n",
        "    config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
        "    self.model = bertDRCD(model_type).to(self.device)\n",
        "    self.model.load_state_dict(torch.load(model_path)) \n",
        "  \n",
        "    # 繁簡轉換\n",
        "    #self.c2tw = OpenCC('s2t') # china to tw\n",
        "    #self.tw2c = OpenCC('t2s') # tw to china\n",
        "\n",
        "  def process(self,content,question):\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      token_tensor = self.tokenizer.encode_plus(str(question),str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
        "      \n",
        "      token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(self.device)\n",
        "      segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(self.device)\n",
        "      mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(self.device)\n",
        "      answer_start,answer_end = self.model(input_ids=token,attention_mask=mask,token_type_ids=segment) \n",
        "      \n",
        "      # get ans\n",
        "      \n",
        "      tokens = self.tokenizer.convert_ids_to_tokens(token.squeeze())\n",
        "      \n",
        "\n",
        "      answer_start = answer_start.argmax(1)\n",
        "      answer_end = answer_end.argmax(1)\n",
        "      print(answer_start,answer_end)\n",
        "      answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "      #print('Answer: \"' + answer + '\"')\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76m8dUy1ew9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = DRCD(f'saved_models/{modelname}',\"bert-base-chinese\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSFpusMof5hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#content = '福茂唱片音樂股份有限公司是一家臺灣唱片公司，成立於1961年，由福茂工程創辦人張人鳳及次子張耕宇先生創立。曾為環球音樂旗下子公司之一。至2002年，正式退出環球音樂旗下，成為獨立唱片公司。成立初期，福茂為英國迪卡唱片在台灣的獨家代理公司，1986年成立國語部，開始發展國語唱片市場，庾澄慶為第一個與福茂唱片簽約的台灣歌手。其後福茂一手捧紅歌手包括庾澄慶、邰正宵、辛曉琪、林隆璇和范曉萱等人。1989年福茂唱片引進CD技術，發行的首張CD專輯是庾澄慶的《讓我一次愛個夠》，銷量40萬張，並入選台灣百佳唱片第48位。1992年，迪卡唱片母公司寶麗金唱片向福茂買入60%股權，福茂也同時加入寶麗金唱片旗下；福茂英文名稱因此改為「Decca Records Taiwan Ltd.」並使用迪卡唱片商標。首張以迪卡唱片商標發行的唱片是庾澄慶首張英文專輯《哈林音樂電台》。在此時期，福茂一手捧紅大陸歌手那英和香港歌手周慧敏、蘇永康、王菲等在台灣的國語市場。1999年，寶麗金唱片集團被現時的環球唱片收購，福茂也曾為環球旗下子公司之一。'\n",
        "content = '2014年世界盃外圍賽，韓國在首輪分組賽以首名出線次輪分組賽，與伊朗、卡達、烏茲別克以及黎巴嫩爭逐兩個直接出線決賽周資格，最後韓國僅以較佳的得失球差壓倒烏茲別克，以小組次名取得2014年世界盃決賽周參賽資格，也是韓國連續八次晉身世界盃決賽周。可惜南韓在決賽周表現不濟，三戰一和兩負小組末席出局。2018年世界盃外圍賽，韓國再次在首輪分組賽以首名出線次輪分組賽，再與伊朗、卡達、烏茲別克同組，同組還有中國及敘利亞。最後韓國以兩分壓倒敘利亞及烏茲別克，再以小組次名取得2018年世界盃決賽周參賽資格，也是韓國連續九次晉身世界盃決賽周。韓國的世界盃成績雖然是亞洲最佳，但在亞洲盃足球賽成績就遠不如世界盃。韓國除了在首兩屆亞洲杯奪冠外，但之後一直與亞洲盃錦標無緣，自1992年至2011年更連續六屆未能打入過亞洲盃決賽。2015年亞洲盃足球賽，韓國以五連勝一球不失的姿態，廿七年來首次打入亞洲盃決賽，對手是東道主澳洲。雖然韓國在分組初賽曾以1-0擊敗澳洲，但這場決賽韓國卻先失一球，最後在下半場補時階段扳平，令比賽進入加時階段，可惜澳洲最後在加時階段攻入致勝一球，最後韓國以1-2敗陣，只得亞軍。'\n",
        "question = \"哪一個國家負責舉辦2015年亞洲盃足球賽?\"\n",
        "e = d.process(content,question)\n",
        "e"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}