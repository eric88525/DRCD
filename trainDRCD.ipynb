{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/eric88525/DRCD/blob/master/trainDRCD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Cn-gQTGhC7z"
   },
   "source": [
    "# DRCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "avAR5VmvfDDg",
    "outputId": "7e106c28-ff76-46de-c53c-53cee945423e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  7 15:49:58 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gdLbhwM65G3"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uubPXZfugyLD",
    "outputId": "a13176bc-25cd-4e8b-c079-ee9e7b8ad9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "import ast\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "from time import strftime,gmtime\n",
    "from opencc import OpenCC\n",
    "import pyprind\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "from zhon.hanzi import non_stops,stops\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5RnZleZyg1jg",
    "outputId": "d0853975-f053-44b6-9001-b6b49eaa8718"
   },
   "outputs": [],
   "source": [
    "#cd ./drive/My Drive/Colab Notebooks/DRCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLaU5RUIqXsl"
   },
   "outputs": [],
   "source": [
    "#!ls\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"MODEL_NAME\")\n",
    "#model = AutoModel.from_pretrained(\"MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TdxaIBxO7uxY"
   },
   "source": [
    "# Get Train Dev Test\n",
    "\n",
    "+ train 8014 26936\n",
    "+ test 1000 3493\n",
    "+ dev 1000 3524\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這是多個答案拆成多組\n",
    "\n",
    "import json\n",
    "\n",
    "class DRCDdataset():\n",
    "  def __init__(self,train_path=None , test_path = None ,dev_path = None):\n",
    "    \n",
    "    if train_path != None:\n",
    "      self.train = self.get_data(train_path)\n",
    "    if test_path != None:\n",
    "      self.test = self.get_data(test_path)\n",
    "    if dev_path != None:\n",
    "      self.dev = self.get_data(dev_path)\n",
    "\n",
    "  def get_data(self,path):\n",
    "    input_file = open(path)\n",
    "    ds = json.load(input_file)\n",
    "    datas = []\n",
    "    for i in range(len(ds['data'])):\n",
    "      for j in ds['data'][i]['paragraphs']:\n",
    "        context = j['context']\n",
    "        for qa in j['qas']:\n",
    "            question = qa['question']\n",
    "            ans = set([ a['text'] for a in  qa['answers']])\n",
    "            for a in ans:\n",
    "                datas.append([context,question,a])\n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yry9u4sI-3zb"
   },
   "outputs": [],
   "source": [
    "# 這是多個答案拆取0\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "class DRCDdataset():\n",
    "  def __init__(self,train_path=None , test_path = None ,dev_path = None):\n",
    "    \n",
    "    if train_path != None:\n",
    "      self.train = self.get_data(train_path)\n",
    "    if test_path != None:\n",
    "      self.test = self.get_data(test_path)\n",
    "    if dev_path != None:\n",
    "      self.dev = self.get_data(dev_path)\n",
    "\n",
    "  def get_data(self,path):\n",
    "    input_file = open(path)\n",
    "    ds = json.load(input_file)\n",
    "    datas = []\n",
    "    for i in range(len(ds['data'])):\n",
    "      for j in ds['data'][i]['paragraphs']:\n",
    "        context = j['context']\n",
    "        question = [q['question'] for q in j['qas']]\n",
    "        ans = [ a['answers'][0]['text'] for a in j['qas'] ]\n",
    "        #ans = [a['answers'][0]['answer_start'] for a in j['qas']]\n",
    "        #anse = [a['answers'][0]['answer_start']+len(a['answers'][0]['text'])-1 for a in j['qas']]\n",
    "        for k in range(len(question)):\n",
    "          datas.append([context,question[k],ans[k]])\n",
    "          #datas.append([context,question[k],ans[k],anse[k]])\n",
    "    return datas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7P7DpSVMcy7Y"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFAFURdNcP77"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class GetDataset(Dataset):\n",
    "    def __init__(self,data,model_type,device,language):\n",
    "      self.data = data\n",
    "      self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "      self.device = device\n",
    "      self.cc = OpenCC('t2s') # tw->china\n",
    "      self.language = language\n",
    "    def __getitem__(self,idx):\n",
    "      paragraph,question,ans = self.data[idx][0] , self.data[idx][1], self.data[idx][2] \n",
    "      if self.language == 'china':\n",
    "        paragraph,question,ans = self.cc.convert(paragraph),self.cc.convert(question),self.cc.convert(ans)\n",
    "        \n",
    "      token_tensor = self.tokenizer.encode_plus(question,paragraph,max_length=512,truncation=True,pad_to_max_length=True)\n",
    "      #print(paragraph[:20],question,ans)\n",
    "      s_idx = [0]*512\n",
    "      e_idx = [0]*512\n",
    "      # 答案tok\n",
    "      s_tok = self.tokenizer.encode(ans)[1:-1]\n",
    "      # 找到開頭跟他一樣的\n",
    "      s_lsit = [i for i, x in enumerate(token_tensor['input_ids']) if x == s_tok[0]]\n",
    "      \n",
    "      for s_pos in s_lsit:\n",
    "        e_pos = s_pos+len(s_tok)\n",
    "        if e_pos > 511:\n",
    "          continue\n",
    "          \n",
    "        if token_tensor['input_ids'][s_pos:e_pos] == s_tok:\n",
    "          s_idx[s_pos] = 1\n",
    "          e_idx[e_pos-1] = 1\n",
    "          break\n",
    "      s_tensor = torch.Tensor(s_idx)\n",
    "      e_tensor = torch.Tensor(e_idx)\n",
    "      # input_ids / token_type_ids / attention_mask / s_tensor / e_tensor\n",
    "      return {'input_ids': torch.tensor(token_tensor['input_ids']).to(self.device)\n",
    "          ,'token_type_ids': torch.tensor( token_tensor['token_type_ids']).to(self.device) \n",
    "          ,'attention_mask': torch.tensor( token_tensor['attention_mask']).to(self.device)\n",
    "          ,'s_tensor': s_tensor.to(self.device)\n",
    "          ,'e_tensor': e_tensor.to(self.device)}\n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "84vSbr-Ynxkp",
    "outputId": "46e83f97-bc8b-402a-d080-cec0da695ceb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nx = DRCDdataset(train_path='./dataset/DRCD_train.json',test_path='./dataset/DRCD_test.json',dev_path='./dataset/DRCD_dev.json')\\nprint(len(x.train),len(x.test),len(x.dev))\\ndevice = torch.device('cuda:0')\\nmodel_type = 'hfl/chinese-roberta-wwm-ext'\\nlanguage = 'china'\\nz = GetDataset(x.dev,model_type,device,language)\\ntokenizer = BertTokenizer.from_pretrained(model_type)\\ni = 10\\n\\ntok = tokenizer.convert_ids_to_tokens(z[i]['input_ids'])\\ns = z[i]['s_tensor'].tolist()\\ne = z[i]['e_tensor'].tolist()\\ntp = z[i]['token_type_ids'].tolist()\\nat = z[i]['attention_mask'].tolist()\\nfor i in range(len(tok)):\\n  print(tok[i],s[i],e[i],tp[i],at[i])\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x = DRCDdataset(train_path='./dataset/DRCD_train.json',test_path='./dataset/DRCD_test.json',dev_path='./dataset/DRCD_dev.json')\n",
    "print(len(x.train),len(x.test),len(x.dev))\n",
    "device = torch.device('cuda:0')\n",
    "model_type = 'hfl/chinese-roberta-wwm-ext'\n",
    "language = 'china'\n",
    "z = GetDataset(x.dev,model_type,device,language)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "i = 10\n",
    "\n",
    "tok = tokenizer.convert_ids_to_tokens(z[i]['input_ids'])\n",
    "s = z[i]['s_tensor'].tolist()\n",
    "e = z[i]['e_tensor'].tolist()\n",
    "tp = z[i]['token_type_ids'].tolist()\n",
    "at = z[i]['attention_mask'].tolist()\n",
    "for i in range(len(tok)):\n",
    "  print(tok[i],s[i],e[i],tp[i],at[i])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Z-zDS64xoOO"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class bertDRCD(nn.Module):\n",
    "    def __init__(self,model_type):\n",
    "        super(bertDRCD,self).__init__()\n",
    "\n",
    "        config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
    "        self.bert_model = BertModel.from_pretrained(model_type,config = config)\n",
    "  \n",
    "        #self.s_vector = nn.Parameter(torch.randn(config.hidden_size),requires_grad=True) \n",
    "        #self.e_vector = nn.Parameter(torch.randn(config.hidden_size),requires_grad=True) \n",
    "\n",
    "        self.s_decoder = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size,1)\n",
    "        ) \n",
    "        self.e_decoder = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size,1)\n",
    "        )\n",
    "        #self.start()\n",
    "\n",
    "    def start(self):      \n",
    "      nn.init.xavier_uniform_(self.s_decoder[0].weight)\n",
    "      nn.init.constant_(self.s_decoder[0].bias, 0) \n",
    "      nn.init.xavier_uniform_(self.e_decoder[0].weight)\n",
    "      nn.init.constant_(self.e_decoder[0].bias, 0)    \n",
    "\n",
    "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None): \n",
    "      # hidden (batch,seq_len,hidden)\n",
    "      hidden = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[0]\n",
    "      s = self.s_decoder(hidden).squeeze()\n",
    "      M = token_type_ids.clone().float().to(hidden.device).detach()\n",
    "      M[M != 1] = float('-inf')\n",
    "      s = s+M \n",
    "      #s = torch.softmax(s,dim=-1)\n",
    "      e = self.e_decoder(hidden).squeeze()\n",
    "      e = e + M\n",
    "      #e = torch.softmax(e,dim=-1)\n",
    "      return s,e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class bertDRCD(nn.Module):\n",
    "    def __init__(self,model_type):\n",
    "        super(bertDRCD,self).__init__()\n",
    "\n",
    "        config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
    "        self.bert_model = BertModel.from_pretrained(model_type,config = config)\n",
    "  \n",
    "        #self.s_vector = nn.Parameter(torch.randn(config.hidden_size),requires_grad=True) \n",
    "        #self.e_vector = nn.Parameter(torch.randn(config.hidden_size),requires_grad=True) \n",
    "\n",
    "        self.s_decoder = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size,config.hidden_size)\n",
    "            ,nn.Dropout(0.1)\n",
    "            ,nn.ReLU()\n",
    "            ,nn.Linear(config.hidden_size,1)\n",
    "        ) \n",
    "        self.e_decoder = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size,config.hidden_size)\n",
    "            ,nn.Dropout(0.1)\n",
    "            ,nn.ReLU()\n",
    "            ,nn.Linear(config.hidden_size,1)\n",
    "        )\n",
    "        #self.start()\n",
    "\n",
    "    def start(self):\n",
    "           \n",
    "      nn.init.xavier_uniform_(self.s_decoder[0].weight)\n",
    "      nn.init.constant_(self.s_decoder[0].bias, 0)\n",
    "      nn.init.xavier_uniform_(self.s_decoder[3].weight)\n",
    "      nn.init.constant_(self.s_decoder[3].bias, 0)\n",
    "      nn.init.xavier_uniform_(self.e_decoder[0].weight)\n",
    "      nn.init.constant_(self.e_decoder[0].bias, 0)\n",
    "      nn.init.xavier_uniform_(self.e_decoder[3].weight)\n",
    "      nn.init.constant_(self.e_decoder[3].bias, 0)\n",
    "\n",
    "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None): \n",
    "      # hidden (batch,seq_len,hidden)\n",
    "      hidden = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[0]\n",
    "      s = self.s_decoder(hidden).squeeze()\n",
    "      M = token_type_ids.clone().float().to(hidden.device).detach()\n",
    "      M[M != 1] = float('-inf')\n",
    "      s = s+M \n",
    "      #s = torch.softmax(s,dim=-1)\n",
    "      e = self.e_decoder(hidden).squeeze()\n",
    "      e = e + M\n",
    "      #e = torch.softmax(e,dim=-1)\n",
    "      return s,e\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AHDfYFexq-Q"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class bertDRCD(nn.Module):\n",
    "    def __init__(self,model_type):\n",
    "        super(bertDRCD,self).__init__()\n",
    "\n",
    "        config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
    "        self.bert_model = BertModel.from_pretrained(model_type,config = config)\n",
    "  \n",
    "        self.s_vector = nn.Parameter(torch.randn(config.hidden_size),requires_grad=True) \n",
    "        self.e_vector = nn.Parameter(torch.randn(config.hidden_size),requires_grad=True) \n",
    "\n",
    "        #self.start()\n",
    "\n",
    "    def start(self):\n",
    "      self.decoder = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size,config.hidden_size)\n",
    "            ,nn.Dropout(0.1)\n",
    "            ,nn.ReLU()\n",
    "            ,nn.Linear(config.hidden_size,1)\n",
    "      )     \n",
    "      nn.init.xavier_uniform_(self.s_decoder[0].weight)\n",
    "      nn.init.constant_(self.s_decoder[0].bias, 0)\n",
    "      nn.init.xavier_uniform_(self.s_decoder[3].weight)\n",
    "      nn.init.constant_(self.s_decoder[3].bias, 0)\n",
    "      nn.init.xavier_uniform_(self.e_decoder[0].weight)\n",
    "      nn.init.constant_(self.e_decoder[0].bias, 0)\n",
    "      nn.init.xavier_uniform_(self.e_decoder[3].weight)\n",
    "      nn.init.constant_(self.e_decoder[3].bias, 0)\n",
    "\n",
    "    def forward(self,input_ids=None,attention_mask=None,token_type_ids=None): \n",
    "      # hidden (batch,seq_len,hidden)\n",
    "      hidden = self.bert_model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)[0]\n",
    "      s = torch.sum(hidden*self.s_vector,-1) # s now is (batch,seq_len)\n",
    "      M = token_type_ids.clone().float().to(hidden.device).detach()\n",
    "      M[M != 1] = float('-inf')\n",
    "      s = s+M \n",
    "      #s = torch.softmax(s,dim=-1)\n",
    "      e = torch.sum(hidden*self.e_vector,-1)\n",
    "      e = e + M\n",
    "      #e = torch.softmax(e,dim=-1)\n",
    "      return s,e\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGgoN41Cwtue"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndevice = torch.device('cpu')\\ndevice = torch.device('cuda:0')\\n\\nmodel_type = 'bert-base-chinese'\\nmodel = bertDRCD(model_type).to(device)\\nds = DRCDdataset(train_path='./dataset/DRCD_train.json',test_path='./dataset/DRCD_test.json',dev_path='./dataset/DRCD_dev.json')\\ndataset = GetDataset(ds.dev,model_type,device)\\ntrainLoader = DataLoader(dataset, batch_size=8  ,shuffle=True)\\ncriterion = nn.BCELoss()\\n\\nfor i , batch in enumerate(trainLoader):\\n  #print(batch)\\n  inp_ids,tok_id,att_m ,slabel,elabel = batch['input_ids'],batch['token_type_ids'],batch['attention_mask'],batch['s_tensor'],batch['e_tensor']\\n  s,e = model(input_ids=inp_ids,attention_mask=att_m,token_type_ids=tok_id)\\n  print(s)\\n  print(e)\\n  loss = (criterion(s,slabel) + criterion(e,elabel)) / 2\\n  print(loss)\\n  \\n  break\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "device = torch.device('cpu')\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "model_type = 'bert-base-chinese'\n",
    "model = bertDRCD(model_type).to(device)\n",
    "ds = DRCDdataset(train_path='./dataset/DRCD_train.json',test_path='./dataset/DRCD_test.json',dev_path='./dataset/DRCD_dev.json')\n",
    "dataset = GetDataset(ds.dev,model_type,device)\n",
    "trainLoader = DataLoader(dataset, batch_size=8  ,shuffle=True)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for i , batch in enumerate(trainLoader):\n",
    "  #print(batch)\n",
    "  inp_ids,tok_id,att_m ,slabel,elabel = batch['input_ids'],batch['token_type_ids'],batch['attention_mask'],batch['s_tensor'],batch['e_tensor']\n",
    "  s,e = model(input_ids=inp_ids,attention_mask=att_m,token_type_ids=tok_id)\n",
    "  print(s)\n",
    "  print(e)\n",
    "  loss = (criterion(s,slabel) + criterion(e,elabel)) / 2\n",
    "  print(loss)\n",
    "  \n",
    "  break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8iCJVtzUVBO"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tNzN6c-xZ_A"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YduImGhxb6q"
   },
   "outputs": [],
   "source": [
    "def test(model,ds,args,tp):\n",
    "    if tp == 'test':\n",
    "        dataset = GetDataset(ds.test,args.model_type,args.device,args.language)\n",
    "        print(f'Testing...{len(dataset)}')\n",
    "    elif tp == 'dev':   \n",
    "        dataset = GetDataset(ds.dev,args.model_type,args.device,args.language)\n",
    "        print(f'Dev...{len(dataset)}')\n",
    "    loader = DataLoader(dataset,batch_size=args.batch_size,shuffle=True)\n",
    "    model.eval()\n",
    "    loss,i = 0,0\n",
    "    print(loss,i)\n",
    "    criterion = nn.BCELoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            i = i+1\n",
    "            inp_ids,tok_id,att_m ,slabel,elabel = batch['input_ids'],batch['token_type_ids'],batch['attention_mask'],batch['s_tensor'],batch['e_tensor']\n",
    "            s,e = model(input_ids=inp_ids,attention_mask=att_m,token_type_ids=tok_id)\n",
    "            #print(f's {s} \\n e {e} \\n sl {torch.softmax(s,dim=-1)} \\n el {torch.softmax(e,dim=-1)} \\n ')\n",
    "            #print(f'SL {slabel} \\n EL {elabel}')\n",
    "            batch_loss = (criterion(torch.softmax(s,dim=-1),slabel) + criterion(torch.softmax(e,dim=-1),elabel)) / 2\n",
    "            loss += batch_loss\n",
    "            \n",
    "        print(f'Result: LOSS: {loss} AVG: {loss/i} ')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYP5p7oaxXQF"
   },
   "source": [
    "# Train\n",
    "+ train(ds,w_d,lr_rate,device,model_type,epoches):\n",
    "+ data(還沒封裝) , weight_d , learningrate , device , model_type , epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGbz1SUhxcb2"
   },
   "outputs": [],
   "source": [
    "def train(ds,args):\n",
    "    dataset = GetDataset(ds.train,args.model_type,args.device,args.language)\n",
    "    trainLoader = DataLoader(dataset,batch_size=args.batch_size,shuffle=True)\n",
    "    model = bertDRCD(args.model_type).to(args.device)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = AdamW(parameters, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    criterion = nn.BCELoss()\n",
    "    model.train()\n",
    "    minloss = 100000\n",
    "    for ei in range(args.epoch):\n",
    "        model.train()\n",
    "        epoch_loss,i,check_loss = 0,0,0\n",
    "        for batch in trainLoader:\n",
    "            i+=1\n",
    "            inp_ids,tok_id,att_m ,slabel,elabel = batch['input_ids'],batch['token_type_ids'],batch['attention_mask'],batch['s_tensor'],batch['e_tensor']\n",
    "            s,e = model(input_ids=inp_ids,attention_mask=att_m,token_type_ids=tok_id)\n",
    "            #print(f's {s} \\n e {e} \\n sl {torch.softmax(s,dim=-1)} \\n el {torch.softmax(e,dim=-1)} \\n ')\n",
    "            #print(f'SL {slabel} \\n EL {elabel}')\n",
    "            batch_loss = (criterion(torch.softmax(s,dim=-1),slabel) + criterion(torch.softmax(e,dim=-1),elabel)) / 2\n",
    "            epoch_loss += batch_loss\n",
    "            check_loss += batch_loss\n",
    "            #print(batch_loss)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if i % 1000==0:\n",
    "                print(f'1000 batch LOSS {check_loss}')\n",
    "                check_loss = 0\n",
    "        dev_loss = test(model,ds,args,'dev') \n",
    "        if dev_loss < minloss:\n",
    "            minloss = dev_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "        print(f'===Epoches: {ei} Loss {epoch_loss}===')       \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llu-ld14gYdA"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x8P9_YXsgaCT",
    "outputId": "b0d26f7b-3f4a-4300-f0a8-0feb6d9e9874"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=4, device=device(type='cuda', index=0), epoch=5, language='tw', learning_rate=1e-05, model_type='hfl/chinese-roberta-wwm-ext', weight_decay=0.001)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser([])\n",
    "parser.add_argument('--batch-size', default=4 , type=int)\n",
    "parser.add_argument('--epoch', default=5, type=int)\n",
    "parser.add_argument('--learning-rate', default=1e-5, type=float)    \n",
    "parser.add_argument('--weight-decay', default=0.001, type=float)\n",
    "parser.add_argument('--model-type', default='hfl/chinese-roberta-wwm-ext' , type=str)  #model_type = 'hfl/chinese-bert-wwm'  'hfl/chinese-roberta-wwm-ext'  'hfl/chinese-roberta-wwm-ext-large'\n",
    "parser.add_argument('--device', default=torch.device('cuda:0'), type=int)\n",
    "parser.add_argument('--language', default='tw', type=str)\n",
    "args = parser.parse_args([])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac4TkXWchw4L"
   },
   "outputs": [],
   "source": [
    "ds = DRCDdataset(train_path='./dataset/DRCD_train.json',test_path='./dataset/DRCD_test.json',dev_path='./dataset/DRCD_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "eHUEmYLLq1C7",
    "outputId": "50541229-7afa-4efe-ea24-a2a3a7330e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2010年引進的廣州快速公交運輸系統，屬世界第二大快速公交系統，日常載客量可達100萬人次，高峰時期每小時單向客流高達26900人次，僅次於波哥大的快速交通系統，平均每10秒鐘就有一輛巴士，每輛巴士單向行駛350小時。包括橋樑在內的站台是世界最長的州快速公交運輸系統站台，長達260米。目前廣州市區的計程車和公共汽車主要使用液化石油氣作燃料，部分公共汽車更使用油電、氣電混合動力技術。2012年底開始投放液化天然氣燃料的公共汽車，2014年6月開始投放液化天然氣插電式混合動力公共汽車，以取代液化石油氣公共汽車。2007年1月16日，廣州市政府全面禁止在市區內駕駛摩托車。違反禁令的機動車將會予以沒收。廣州市交通局聲稱禁令的施行，使得交通擁擠問題和車禍大幅減少。廣州白雲國際機場位於白雲區與花都區交界，2004年8月5日正式投入運營，屬中國交通情況第二繁忙的機場。該機場取代了原先位於市中心的無法滿足日益增長航空需求的舊機場。目前機場有三條飛機跑道，成為國內第三個擁有三跑道的民航機場。比鄰近的香港國際機場第三跑道預計的2023年落成早8年。',\n",
       "  '廣州的快速公交運輸系統每多久就會有一輛巴士？',\n",
       "  '10秒鐘'],\n",
       " ['2010年引進的廣州快速公交運輸系統，屬世界第二大快速公交系統，日常載客量可達100萬人次，高峰時期每小時單向客流高達26900人次，僅次於波哥大的快速交通系統，平均每10秒鐘就有一輛巴士，每輛巴士單向行駛350小時。包括橋樑在內的站台是世界最長的州快速公交運輸系統站台，長達260米。目前廣州市區的計程車和公共汽車主要使用液化石油氣作燃料，部分公共汽車更使用油電、氣電混合動力技術。2012年底開始投放液化天然氣燃料的公共汽車，2014年6月開始投放液化天然氣插電式混合動力公共汽車，以取代液化石油氣公共汽車。2007年1月16日，廣州市政府全面禁止在市區內駕駛摩托車。違反禁令的機動車將會予以沒收。廣州市交通局聲稱禁令的施行，使得交通擁擠問題和車禍大幅減少。廣州白雲國際機場位於白雲區與花都區交界，2004年8月5日正式投入運營，屬中國交通情況第二繁忙的機場。該機場取代了原先位於市中心的無法滿足日益增長航空需求的舊機場。目前機場有三條飛機跑道，成為國內第三個擁有三跑道的民航機場。比鄰近的香港國際機場第三跑道預計的2023年落成早8年。',\n",
       "  '從哪一天開始在廣州市內騎摩托車會被沒收？',\n",
       "  '2007年1月16日'],\n",
       " ['2010年引進的廣州快速公交運輸系統，屬世界第二大快速公交系統，日常載客量可達100萬人次，高峰時期每小時單向客流高達26900人次，僅次於波哥大的快速交通系統，平均每10秒鐘就有一輛巴士，每輛巴士單向行駛350小時。包括橋樑在內的站台是世界最長的州快速公交運輸系統站台，長達260米。目前廣州市區的計程車和公共汽車主要使用液化石油氣作燃料，部分公共汽車更使用油電、氣電混合動力技術。2012年底開始投放液化天然氣燃料的公共汽車，2014年6月開始投放液化天然氣插電式混合動力公共汽車，以取代液化石油氣公共汽車。2007年1月16日，廣州市政府全面禁止在市區內駕駛摩托車。違反禁令的機動車將會予以沒收。廣州市交通局聲稱禁令的施行，使得交通擁擠問題和車禍大幅減少。廣州白雲國際機場位於白雲區與花都區交界，2004年8月5日正式投入運營，屬中國交通情況第二繁忙的機場。該機場取代了原先位於市中心的無法滿足日益增長航空需求的舊機場。目前機場有三條飛機跑道，成為國內第三個擁有三跑道的民航機場。比鄰近的香港國際機場第三跑道預計的2023年落成早8年。',\n",
       "  '廣州白雲國際機場在完成第三條跑道的後八年哪一座機場也會有第三跑道？',\n",
       "  '香港國際機場'],\n",
       " ['廣州是京廣鐵路、廣深鐵路、廣茂鐵路、廣梅汕鐵路的終點站。2009年末，武廣客運專線投入運營，多單元列車覆蓋980公里的路程，最高時速可達350公里/小時。2011年1月7日，廣珠城際鐵路投入運營，平均時速可達200公里/小時。廣州鐵路、長途汽車和渡輪直達香港，廣九直通車從廣州東站開出，直達香港九龍紅磡站，總長度約182公里，車程在兩小時內。繁忙的長途汽車每年會從城市中的不同載客點把旅客接載至香港。在珠江靠市中心的北航道有渡輪線路，用於近江居民直接渡江而無需乘坐公交或步行過橋。南沙碼頭和蓮花山碼頭間每天都有高速雙體船往返，渡輪也開往香港中國客運碼頭和港澳碼頭。',\n",
       "  '廣珠城際鐵路平均每小時可以走多遠？',\n",
       "  '200公里'],\n",
       " ['廣州是京廣鐵路、廣深鐵路、廣茂鐵路、廣梅汕鐵路的終點站。2009年末，武廣客運專線投入運營，多單元列車覆蓋980公里的路程，最高時速可達350公里/小時。2011年1月7日，廣珠城際鐵路投入運營，平均時速可達200公里/小時。廣州鐵路、長途汽車和渡輪直達香港，廣九直通車從廣州東站開出，直達香港九龍紅磡站，總長度約182公里，車程在兩小時內。繁忙的長途汽車每年會從城市中的不同載客點把旅客接載至香港。在珠江靠市中心的北航道有渡輪線路，用於近江居民直接渡江而無需乘坐公交或步行過橋。南沙碼頭和蓮花山碼頭間每天都有高速雙體船往返，渡輪也開往香港中國客運碼頭和港澳碼頭。',\n",
       "  '廣九直通車從頭坐到尾約需要多久？',\n",
       "  '兩小時'],\n",
       " ['廣州是京廣鐵路、廣深鐵路、廣茂鐵路、廣梅汕鐵路的終點站。2009年末，武廣客運專線投入運營，多單元列車覆蓋980公里的路程，最高時速可達350公里/小時。2011年1月7日，廣珠城際鐵路投入運營，平均時速可達200公里/小時。廣州鐵路、長途汽車和渡輪直達香港，廣九直通車從廣州東站開出，直達香港九龍紅磡站，總長度約182公里，車程在兩小時內。繁忙的長途汽車每年會從城市中的不同載客點把旅客接載至香港。在珠江靠市中心的北航道有渡輪線路，用於近江居民直接渡江而無需乘坐公交或步行過橋。南沙碼頭和蓮花山碼頭間每天都有高速雙體船往返，渡輪也開往香港中國客運碼頭和港澳碼頭。',\n",
       "  '讓近江居民可以直接渡江而不需要步行過橋是因為甚麼？',\n",
       "  '渡輪線路'],\n",
       " ['廣州自古已是華南地區著名的商埠，擁有2000多年的開放貿易歷史。1970年代末中國大陸改革開放後，廣州經濟發展迅速。2010年全市地區生產總值為10604.48億元人民幣，同比增長13%，成為繼上海、北京之後第三個進入國內生產總值「萬億元俱樂部」的城市，也是首個經濟總量過萬億的省會城市。根據國務院2005年發表的一份報告稱，廣州成為中國第一個進入「發達」狀態的城市。2012年9月，廣州南沙新區獲批，成為第六個國家級開放開發新區。2015廣州市國內生產總值達到18100.41億元，人均國內生產總值達138377.05元。國內生產總值總量為中國第三，人均國內生產總值水平與西班牙相當，購買力平價水準已和已開發國家的中心城市相當。',\n",
       "  '進入國內生產總值「萬億元俱樂部」的城市第三個為？',\n",
       "  '廣州'],\n",
       " ['廣州自古已是華南地區著名的商埠，擁有2000多年的開放貿易歷史。1970年代末中國大陸改革開放後，廣州經濟發展迅速。2010年全市地區生產總值為10604.48億元人民幣，同比增長13%，成為繼上海、北京之後第三個進入國內生產總值「萬億元俱樂部」的城市，也是首個經濟總量過萬億的省會城市。根據國務院2005年發表的一份報告稱，廣州成為中國第一個進入「發達」狀態的城市。2012年9月，廣州南沙新區獲批，成為第六個國家級開放開發新區。2015廣州市國內生產總值達到18100.41億元，人均國內生產總值達138377.05元。國內生產總值總量為中國第三，人均國內生產總值水平與西班牙相當，購買力平價水準已和已開發國家的中心城市相當。',\n",
       "  '中國第一個進入「發達」狀態的城市為？',\n",
       "  '廣州'],\n",
       " ['廣州自古已是華南地區著名的商埠，擁有2000多年的開放貿易歷史。1970年代末中國大陸改革開放後，廣州經濟發展迅速。2010年全市地區生產總值為10604.48億元人民幣，同比增長13%，成為繼上海、北京之後第三個進入國內生產總值「萬億元俱樂部」的城市，也是首個經濟總量過萬億的省會城市。根據國務院2005年發表的一份報告稱，廣州成為中國第一個進入「發達」狀態的城市。2012年9月，廣州南沙新區獲批，成為第六個國家級開放開發新區。2015廣州市國內生產總值達到18100.41億元，人均國內生產總值達138377.05元。國內生產總值總量為中國第三，人均國內生產總值水平與西班牙相當，購買力平價水準已和已開發國家的中心城市相當。',\n",
       "  '和西班牙的人均國內生產總值水平相當的中國城市為？',\n",
       "  '廣州市'],\n",
       " ['廣州市內雨水充潤、土地肥沃，市區曾經有非常廣大的農業用地。兩千年前就已經有水稻種植的記載。宋代的廣州是中國最大的米市之一，蔬菜、水果、糖蔗、花卉也享有盛名。由於曾長久作為全國性港口，廣州引進了多種優良作物品種。二十世紀上半葉，交通發展造成外地農產品入侵以及戰爭的影響下，廣州農業增長緩滯。二十世紀五十年代初中期農業產量大幅增長，但後期的「大躍進」以及其後的文革嚴重打擊了農業生產。改革開放後，隨著廣州發展為大城市，逐漸形成了為城市服務的城郊農業格局，政府的方針為服務城市，富農利民，鼓勵出口創匯。具體措施為降低產糧比例，增加蔬果蛋奶等農副產品比例，致力發展林牧漁業。廣州農業產值大幅增長，但隨著第二第三產業飛速增長，農業所占的經濟比重仍逐漸降。1978年廣州市農業生產總值占地區生總值的11.67％，1990年下降至8.05％，2015年，廣州市農業總產值413億元，占地區生產總值的2.28%。城市發展和工業化進程也使得農村勞動力大量流失，農耕用地迅速萎縮。清代光緒五年廣州府登記在冊的田地山塘總面積為1062.32萬畝。1990年的耕地面積為247萬畝，2006年減至158.3萬畝。目前，廣州種植農産品的地方主要在白雲、花都、番禺、南沙、從化和增城。1990年農業人口有252.87萬人，2008年為79.22萬人。廣州較為著名的農業特產有泮塘五秀，以及以荔枝、香蕉、木瓜、菠蘿為首的四大嶺南佳果等各類熱帶水果。',\n",
       "  '廣州能夠引進了許多優良作物的品種的原因為？',\n",
       "  '曾長久作為全國性港口'],\n",
       " ['廣州市內雨水充潤、土地肥沃，市區曾經有非常廣大的農業用地。兩千年前就已經有水稻種植的記載。宋代的廣州是中國最大的米市之一，蔬菜、水果、糖蔗、花卉也享有盛名。由於曾長久作為全國性港口，廣州引進了多種優良作物品種。二十世紀上半葉，交通發展造成外地農產品入侵以及戰爭的影響下，廣州農業增長緩滯。二十世紀五十年代初中期農業產量大幅增長，但後期的「大躍進」以及其後的文革嚴重打擊了農業生產。改革開放後，隨著廣州發展為大城市，逐漸形成了為城市服務的城郊農業格局，政府的方針為服務城市，富農利民，鼓勵出口創匯。具體措施為降低產糧比例，增加蔬果蛋奶等農副產品比例，致力發展林牧漁業。廣州農業產值大幅增長，但隨著第二第三產業飛速增長，農業所占的經濟比重仍逐漸降。1978年廣州市農業生產總值占地區生總值的11.67％，1990年下降至8.05％，2015年，廣州市農業總產值413億元，占地區生產總值的2.28%。城市發展和工業化進程也使得農村勞動力大量流失，農耕用地迅速萎縮。清代光緒五年廣州府登記在冊的田地山塘總面積為1062.32萬畝。1990年的耕地面積為247萬畝，2006年減至158.3萬畝。目前，廣州種植農産品的地方主要在白雲、花都、番禺、南沙、從化和增城。1990年農業人口有252.87萬人，2008年為79.22萬人。廣州較為著名的農業特產有泮塘五秀，以及以荔枝、香蕉、木瓜、菠蘿為首的四大嶺南佳果等各類熱帶水果。',\n",
       "  '1978年的廣州市農業生產總值到了哪一年占地區生總值減少了約9.39%？',\n",
       "  '2015'],\n",
       " ['廣州市內雨水充潤、土地肥沃，市區曾經有非常廣大的農業用地。兩千年前就已經有水稻種植的記載。宋代的廣州是中國最大的米市之一，蔬菜、水果、糖蔗、花卉也享有盛名。由於曾長久作為全國性港口，廣州引進了多種優良作物品種。二十世紀上半葉，交通發展造成外地農產品入侵以及戰爭的影響下，廣州農業增長緩滯。二十世紀五十年代初中期農業產量大幅增長，但後期的「大躍進」以及其後的文革嚴重打擊了農業生產。改革開放後，隨著廣州發展為大城市，逐漸形成了為城市服務的城郊農業格局，政府的方針為服務城市，富農利民，鼓勵出口創匯。具體措施為降低產糧比例，增加蔬果蛋奶等農副產品比例，致力發展林牧漁業。廣州農業產值大幅增長，但隨著第二第三產業飛速增長，農業所占的經濟比重仍逐漸降。1978年廣州市農業生產總值占地區生總值的11.67％，1990年下降至8.05％，2015年，廣州市農業總產值413億元，占地區生產總值的2.28%。城市發展和工業化進程也使得農村勞動力大量流失，農耕用地迅速萎縮。清代光緒五年廣州府登記在冊的田地山塘總面積為1062.32萬畝。1990年的耕地面積為247萬畝，2006年減至158.3萬畝。目前，廣州種植農産品的地方主要在白雲、花都、番禺、南沙、從化和增城。1990年農業人口有252.87萬人，2008年為79.22萬人。廣州較為著名的農業特產有泮塘五秀，以及以荔枝、香蕉、木瓜、菠蘿為首的四大嶺南佳果等各類熱帶水果。',\n",
       "  '廣州1990年的農業人口到了哪一年減少了約174萬人？',\n",
       "  '2008'],\n",
       " ['古代廣州的手工業非常發達，船舶業、冶鑄和五金業、紡織業、食品加工、中成藥業、陶瓷業、美工等都享譽全國。廣州清初所產棉花，以輕暖出名，號「廣花」；「廣之線紗」，於廣州十三行之財富積累也有助推。由於本埠棉布價廉物美於英國布，出口甚多。在巨大利益驅使下，棉花來料加工迅速興起，西關農田被大量開發，修築廠房街道，錦華、經綸、麻紗等地方便於此時期出現。廣州紡織業帶之形成，帶動了印染、機具、漿緞、製衣、制帽、鞋襪、絨線等行業的興盛。下西關涌郊區也被發展為高尚住宅區，業主有不少是因紡織業大旺而獲益的洋行買辦。西關在當時有所謂「八橋之盛」。十九世紀七十年代起清廷展開洋務運動，廣州近代工業開始起步。官辦企業有廣州機器局製造洋槍彈藥和輪船，廣東錢局則是國內首家以機器鑄幣的企業。民辦企業包括繼昌隆繅絲廠、泰安大藥房、華興織造總公司等。清朝末期，廣州附近已經集中了大量各種輕工業工廠。民國開始，海外華僑「實業救國」的思潮帶動了廣州輕工業快速發展，對外貿易大幅增長，使廣州成為當時中國較為發達的城市，同時吸引不少外地人到此創業。1929年至1936年，陳濟棠主粵時利用南方相對穩定的政經環境與世界性經濟衰退的局勢，建立了較完善的工業體系，對廣東經濟起到極大的推動作用。抗戰期間，廣州被日軍占領，工業遭受徹底破壞。內戰期間，物價飛漲，大半工廠停工或半停工，失業者眾。',\n",
       "  '廣州清初產棉花由於和原因而出口甚多？',\n",
       "  '本埠棉布價廉物美於英國布'],\n",
       " ['古代廣州的手工業非常發達，船舶業、冶鑄和五金業、紡織業、食品加工、中成藥業、陶瓷業、美工等都享譽全國。廣州清初所產棉花，以輕暖出名，號「廣花」；「廣之線紗」，於廣州十三行之財富積累也有助推。由於本埠棉布價廉物美於英國布，出口甚多。在巨大利益驅使下，棉花來料加工迅速興起，西關農田被大量開發，修築廠房街道，錦華、經綸、麻紗等地方便於此時期出現。廣州紡織業帶之形成，帶動了印染、機具、漿緞、製衣、制帽、鞋襪、絨線等行業的興盛。下西關涌郊區也被發展為高尚住宅區，業主有不少是因紡織業大旺而獲益的洋行買辦。西關在當時有所謂「八橋之盛」。十九世紀七十年代起清廷展開洋務運動，廣州近代工業開始起步。官辦企業有廣州機器局製造洋槍彈藥和輪船，廣東錢局則是國內首家以機器鑄幣的企業。民辦企業包括繼昌隆繅絲廠、泰安大藥房、華興織造總公司等。清朝末期，廣州附近已經集中了大量各種輕工業工廠。民國開始，海外華僑「實業救國」的思潮帶動了廣州輕工業快速發展，對外貿易大幅增長，使廣州成為當時中國較為發達的城市，同時吸引不少外地人到此創業。1929年至1936年，陳濟棠主粵時利用南方相對穩定的政經環境與世界性經濟衰退的局勢，建立了較完善的工業體系，對廣東經濟起到極大的推動作用。抗戰期間，廣州被日軍占領，工業遭受徹底破壞。內戰期間，物價飛漲，大半工廠停工或半停工，失業者眾。',\n",
       "  '民國一開始廣州輕工業快速發展是由於甚麼思想？',\n",
       "  '實業救國'],\n",
       " ['古代廣州的手工業非常發達，船舶業、冶鑄和五金業、紡織業、食品加工、中成藥業、陶瓷業、美工等都享譽全國。廣州清初所產棉花，以輕暖出名，號「廣花」；「廣之線紗」，於廣州十三行之財富積累也有助推。由於本埠棉布價廉物美於英國布，出口甚多。在巨大利益驅使下，棉花來料加工迅速興起，西關農田被大量開發，修築廠房街道，錦華、經綸、麻紗等地方便於此時期出現。廣州紡織業帶之形成，帶動了印染、機具、漿緞、製衣、制帽、鞋襪、絨線等行業的興盛。下西關涌郊區也被發展為高尚住宅區，業主有不少是因紡織業大旺而獲益的洋行買辦。西關在當時有所謂「八橋之盛」。十九世紀七十年代起清廷展開洋務運動，廣州近代工業開始起步。官辦企業有廣州機器局製造洋槍彈藥和輪船，廣東錢局則是國內首家以機器鑄幣的企業。民辦企業包括繼昌隆繅絲廠、泰安大藥房、華興織造總公司等。清朝末期，廣州附近已經集中了大量各種輕工業工廠。民國開始，海外華僑「實業救國」的思潮帶動了廣州輕工業快速發展，對外貿易大幅增長，使廣州成為當時中國較為發達的城市，同時吸引不少外地人到此創業。1929年至1936年，陳濟棠主粵時利用南方相對穩定的政經環境與世界性經濟衰退的局勢，建立了較完善的工業體系，對廣東經濟起到極大的推動作用。抗戰期間，廣州被日軍占領，工業遭受徹底破壞。內戰期間，物價飛漲，大半工廠停工或半停工，失業者眾。',\n",
       "  '是誰在廣州建立了完善的工業體系對其經濟起到很大的推動作用？',\n",
       "  '陳濟棠'],\n",
       " ['中華人民共和國成立後工業國有化。五六十年代時，工業有所恢復，但文化大革命再次嚴重衝擊廣州工業生產，工業發展減慢。但其間仍在政府扶持下建立了重工業體系。1975年後，政治局面好轉，政府大力扶持日用品為主的輕工業，廣州工業進入快速增長時期。八十年代後期，廣州市主要發展第三產業，工業產值比重下降。九十年代起乃至2000年以後，廣州市政府力圖改變產業構成，加大了對重工業扶植。汽車、石化和電子信息產品製造已成為廣州三大支柱產業。廣州工業總產值位居全省第三位。其中汽車產業方面，相繼成功吸引日本三間主要汽車製造企業投資設立工廠，令廣州在數年間成為中國重要的汽車生產基地之一，江南最大的廣州市陳田汽配市場位於市郊，廣州同時亦是國內出產日本汽車最多的城市。',\n",
       "  '中華人民共和國成立後由於什麼事件廣州的工業發展又減慢？',\n",
       "  '文化大革命'],\n",
       " ['中華人民共和國成立後工業國有化。五六十年代時，工業有所恢復，但文化大革命再次嚴重衝擊廣州工業生產，工業發展減慢。但其間仍在政府扶持下建立了重工業體系。1975年後，政治局面好轉，政府大力扶持日用品為主的輕工業，廣州工業進入快速增長時期。八十年代後期，廣州市主要發展第三產業，工業產值比重下降。九十年代起乃至2000年以後，廣州市政府力圖改變產業構成，加大了對重工業扶植。汽車、石化和電子信息產品製造已成為廣州三大支柱產業。廣州工業總產值位居全省第三位。其中汽車產業方面，相繼成功吸引日本三間主要汽車製造企業投資設立工廠，令廣州在數年間成為中國重要的汽車生產基地之一，江南最大的廣州市陳田汽配市場位於市郊，廣州同時亦是國內出產日本汽車最多的城市。',\n",
       "  '到了什麼時期廣州的工業產業比重開始下降？',\n",
       "  '八十年代後期'],\n",
       " ['中華人民共和國成立後工業國有化。五六十年代時，工業有所恢復，但文化大革命再次嚴重衝擊廣州工業生產，工業發展減慢。但其間仍在政府扶持下建立了重工業體系。1975年後，政治局面好轉，政府大力扶持日用品為主的輕工業，廣州工業進入快速增長時期。八十年代後期，廣州市主要發展第三產業，工業產值比重下降。九十年代起乃至2000年以後，廣州市政府力圖改變產業構成，加大了對重工業扶植。汽車、石化和電子信息產品製造已成為廣州三大支柱產業。廣州工業總產值位居全省第三位。其中汽車產業方面，相繼成功吸引日本三間主要汽車製造企業投資設立工廠，令廣州在數年間成為中國重要的汽車生產基地之一，江南最大的廣州市陳田汽配市場位於市郊，廣州同時亦是國內出產日本汽車最多的城市。',\n",
       "  '中國國內出產日本汽車最多的城市為？',\n",
       "  '廣州'],\n",
       " ['六朝時期的廣州對外貿易已相當興旺，外國海商「久停廣州，往來求利」。隋唐時期廣州對外貿易發展到一個頂峰，作為唐朝唯一設置市舶使的城市，外國人數量一度達到全城人口的30%以上，成為當時中國對外貿易的核心。經過元朝的短暫沉寂後，明清兩朝廣州再次崛起，在清朝一口通商政策下成為當時中國唯一的對外口岸，成為具有壟斷地位的全國商業中心。上海開埠後，隨著全國逐漸開放，廣州逐漸失去對外貿易中心地位，但仍然是全國最重要的商業城市。改革開放後的廣州百貨業蓬勃發展，1980年代起友誼商店專門為港澳及外賓銷售電器等入口商品，西湖路燈光夜市則是平民的熱門去處。而地處沿江西路江邊的南方大廈就成為華南地區規模最大的綜合性百貨商店，更創立內地第一家24小時便利店，和華夏百貨形成人民南商圈，是上下九傳統西部商圈的延伸，後者在1995年成立步行街。位於北京路的新大新公司和廣州百貨大廈業務亦蒸蒸日上，馬路後來也闢為步行街。超級市場萬客隆1996年在廣州開設內地第一家分店，隨後香港百佳超級市場等廣州人熟悉的超市及便利店品牌陸續進入廣州。因受交通壓力及天河新區發展影響，90年代尾的人民南商圈開始衰落，成為電子服裝批發集散地，城市商圈向東部新區轉移。到21世紀，以天河城、天環廣場、正佳廣場、太古匯、萬菱匯為代表的天河路商圈成為廣州的中心商圈。自1957年起每年舉辦春、秋兩屆的中國出口商品交易會，現時每屆展會均吸引來自全球各地逾20萬客商，是中國規模最大、時間最長、最成功的國際展會。隨著琶洲展館三期的建設，展會能力躍居世界第一。',\n",
       "  '唐朝的對外貿易核心在哪？',\n",
       "  '廣州'],\n",
       " ['六朝時期的廣州對外貿易已相當興旺，外國海商「久停廣州，往來求利」。隋唐時期廣州對外貿易發展到一個頂峰，作為唐朝唯一設置市舶使的城市，外國人數量一度達到全城人口的30%以上，成為當時中國對外貿易的核心。經過元朝的短暫沉寂後，明清兩朝廣州再次崛起，在清朝一口通商政策下成為當時中國唯一的對外口岸，成為具有壟斷地位的全國商業中心。上海開埠後，隨著全國逐漸開放，廣州逐漸失去對外貿易中心地位，但仍然是全國最重要的商業城市。改革開放後的廣州百貨業蓬勃發展，1980年代起友誼商店專門為港澳及外賓銷售電器等入口商品，西湖路燈光夜市則是平民的熱門去處。而地處沿江西路江邊的南方大廈就成為華南地區規模最大的綜合性百貨商店，更創立內地第一家24小時便利店，和華夏百貨形成人民南商圈，是上下九傳統西部商圈的延伸，後者在1995年成立步行街。位於北京路的新大新公司和廣州百貨大廈業務亦蒸蒸日上，馬路後來也闢為步行街。超級市場萬客隆1996年在廣州開設內地第一家分店，隨後香港百佳超級市場等廣州人熟悉的超市及便利店品牌陸續進入廣州。因受交通壓力及天河新區發展影響，90年代尾的人民南商圈開始衰落，成為電子服裝批發集散地，城市商圈向東部新區轉移。到21世紀，以天河城、天環廣場、正佳廣場、太古匯、萬菱匯為代表的天河路商圈成為廣州的中心商圈。自1957年起每年舉辦春、秋兩屆的中國出口商品交易會，現時每屆展會均吸引來自全球各地逾20萬客商，是中國規模最大、時間最長、最成功的國際展會。隨著琶洲展館三期的建設，展會能力躍居世界第一。',\n",
       "  '廣州的第一家24小時便利商店是在什麼時期創立的？',\n",
       "  '1980年代'],\n",
       " ['六朝時期的廣州對外貿易已相當興旺，外國海商「久停廣州，往來求利」。隋唐時期廣州對外貿易發展到一個頂峰，作為唐朝唯一設置市舶使的城市，外國人數量一度達到全城人口的30%以上，成為當時中國對外貿易的核心。經過元朝的短暫沉寂後，明清兩朝廣州再次崛起，在清朝一口通商政策下成為當時中國唯一的對外口岸，成為具有壟斷地位的全國商業中心。上海開埠後，隨著全國逐漸開放，廣州逐漸失去對外貿易中心地位，但仍然是全國最重要的商業城市。改革開放後的廣州百貨業蓬勃發展，1980年代起友誼商店專門為港澳及外賓銷售電器等入口商品，西湖路燈光夜市則是平民的熱門去處。而地處沿江西路江邊的南方大廈就成為華南地區規模最大的綜合性百貨商店，更創立內地第一家24小時便利店，和華夏百貨形成人民南商圈，是上下九傳統西部商圈的延伸，後者在1995年成立步行街。位於北京路的新大新公司和廣州百貨大廈業務亦蒸蒸日上，馬路後來也闢為步行街。超級市場萬客隆1996年在廣州開設內地第一家分店，隨後香港百佳超級市場等廣州人熟悉的超市及便利店品牌陸續進入廣州。因受交通壓力及天河新區發展影響，90年代尾的人民南商圈開始衰落，成為電子服裝批發集散地，城市商圈向東部新區轉移。到21世紀，以天河城、天環廣場、正佳廣場、太古匯、萬菱匯為代表的天河路商圈成為廣州的中心商圈。自1957年起每年舉辦春、秋兩屆的中國出口商品交易會，現時每屆展會均吸引來自全球各地逾20萬客商，是中國規模最大、時間最長、最成功的國際展會。隨著琶洲展館三期的建設，展會能力躍居世界第一。',\n",
       "  '中國出口商品交易會現在每次舉辦參加的客商都有約多少？',\n",
       "  '20萬'],\n",
       " ['從古至今，廣州基本上是嶺南地區的政治中心。秦末為南越國都城，漢朝征服南越國後立番禺為南海郡治。漢末郡治遷至龍灣與古壩之間。三國時，吳國步騭將郡治遷回番禺，後又設為交州治所。交廣分治後為廣州治所。晉、南北朝沿用南海郡，番禺為郡治。隋文帝廢南海郡，置廣州總管府，後改為都督府。唐代分全國為十道，其中嶺南道治所設在廣州。862年，嶺南分東、西二道，廣州為嶺南東道治。五代時期廣州為南漢國國都。宋初復設嶺南道，廣州為治所。明朝廣州為廣州府城，由番禺縣、南海縣兩縣分管，亦為廣東承宣布政使司駐地。清軍占領中原後，南明紹武帝朱聿讁在廣州建都，不久南明滅亡，清朝的廣州成為廣東省會、廣州府城及番禺與南海縣治。兩廣總督衙門、廣東巡撫衙門、廣東布政司、廣東按察司都在廣州南海縣界內。1858年，英法聯軍攻陷廣州，廣東巡撫投降，外人委員會成立，廣東政府成為傀儡政府。',\n",
       "  '把南海郡廢除改置廣州總管府的皇帝是誰？',\n",
       "  '隋文帝'],\n",
       " ['從古至今，廣州基本上是嶺南地區的政治中心。秦末為南越國都城，漢朝征服南越國後立番禺為南海郡治。漢末郡治遷至龍灣與古壩之間。三國時，吳國步騭將郡治遷回番禺，後又設為交州治所。交廣分治後為廣州治所。晉、南北朝沿用南海郡，番禺為郡治。隋文帝廢南海郡，置廣州總管府，後改為都督府。唐代分全國為十道，其中嶺南道治所設在廣州。862年，嶺南分東、西二道，廣州為嶺南東道治。五代時期廣州為南漢國國都。宋初復設嶺南道，廣州為治所。明朝廣州為廣州府城，由番禺縣、南海縣兩縣分管，亦為廣東承宣布政使司駐地。清軍占領中原後，南明紹武帝朱聿讁在廣州建都，不久南明滅亡，清朝的廣州成為廣東省會、廣州府城及番禺與南海縣治。兩廣總督衙門、廣東巡撫衙門、廣東布政司、廣東按察司都在廣州南海縣界內。1858年，英法聯軍攻陷廣州，廣東巡撫投降，外人委員會成立，廣東政府成為傀儡政府。',\n",
       "  '南明的首都在哪？',\n",
       "  '廣州'],\n",
       " ['從古至今，廣州基本上是嶺南地區的政治中心。秦末為南越國都城，漢朝征服南越國後立番禺為南海郡治。漢末郡治遷至龍灣與古壩之間。三國時，吳國步騭將郡治遷回番禺，後又設為交州治所。交廣分治後為廣州治所。晉、南北朝沿用南海郡，番禺為郡治。隋文帝廢南海郡，置廣州總管府，後改為都督府。唐代分全國為十道，其中嶺南道治所設在廣州。862年，嶺南分東、西二道，廣州為嶺南東道治。五代時期廣州為南漢國國都。宋初復設嶺南道，廣州為治所。明朝廣州為廣州府城，由番禺縣、南海縣兩縣分管，亦為廣東承宣布政使司駐地。清軍占領中原後，南明紹武帝朱聿讁在廣州建都，不久南明滅亡，清朝的廣州成為廣東省會、廣州府城及番禺與南海縣治。兩廣總督衙門、廣東巡撫衙門、廣東布政司、廣東按察司都在廣州南海縣界內。1858年，英法聯軍攻陷廣州，廣東巡撫投降，外人委員會成立，廣東政府成為傀儡政府。',\n",
       "  '廣東政府在甚麼時候成為傀儡政府？',\n",
       "  '1858年'],\n",
       " ['清朝末期，廣州爆發了數次武裝起義，均以失敗告終。1911年10月10日武昌起義後，廣東省獨立，11月10日成立軍政府，推選胡漢民為都督。12月初，廣東臨時省議會成立，公布21歲以上廣東籍人皆有選舉權和被選舉權，議會由120名議員組成，其中同盟會代表20名，軍團協會代表21名，華僑代表12名，師生代表9名，「自治團」代表1名，各地區代表57名。其中女性議員須占10名，開中國婦女參政先河。1913年4月27日，廣東省議會成立，羅曉峰任議長。1918年10月，廣州市市政公所成立。1921年2月廣州正式設市，成立廣州市政廳，孫科任市長。1917年至1922年間，孫中山以及西南各省的國會代表兩次在廣州成立護法軍政府。1925年孫中山逝世，同年7月廣州國民政府成立，7月4日成立廣州市市政府，實行市政委員會制，伍朝樞任市政委員會委員長。1929年，廣州市實行市長制。1949年10月14日中國人民解放軍占領廣州，28日成立廣州市軍事管理委員會，葉劍英任主席。目前廣州市在中華人民共和國政制架構下實行人民代表大會制度，市政府在中共廣州市委的領導下運作，政府駐地越秀區。作為廣東省的省會，廣州市是廣東省人民政府駐地。',\n",
       "  '廣東省因為哪一場戰爭而獨立？',\n",
       "  '武昌起義'],\n",
       " ['清朝末期，廣州爆發了數次武裝起義，均以失敗告終。1911年10月10日武昌起義後，廣東省獨立，11月10日成立軍政府，推選胡漢民為都督。12月初，廣東臨時省議會成立，公布21歲以上廣東籍人皆有選舉權和被選舉權，議會由120名議員組成，其中同盟會代表20名，軍團協會代表21名，華僑代表12名，師生代表9名，「自治團」代表1名，各地區代表57名。其中女性議員須占10名，開中國婦女參政先河。1913年4月27日，廣東省議會成立，羅曉峰任議長。1918年10月，廣州市市政公所成立。1921年2月廣州正式設市，成立廣州市政廳，孫科任市長。1917年至1922年間，孫中山以及西南各省的國會代表兩次在廣州成立護法軍政府。1925年孫中山逝世，同年7月廣州國民政府成立，7月4日成立廣州市市政府，實行市政委員會制，伍朝樞任市政委員會委員長。1929年，廣州市實行市長制。1949年10月14日中國人民解放軍占領廣州，28日成立廣州市軍事管理委員會，葉劍英任主席。目前廣州市在中華人民共和國政制架構下實行人民代表大會制度，市政府在中共廣州市委的領導下運作，政府駐地越秀區。作為廣東省的省會，廣州市是廣東省人民政府駐地。',\n",
       "  '孫中山死掉約24年後廣州被甚麼佔領了？',\n",
       "  '中國人民解放軍'],\n",
       " ['清朝末期，廣州爆發了數次武裝起義，均以失敗告終。1911年10月10日武昌起義後，廣東省獨立，11月10日成立軍政府，推選胡漢民為都督。12月初，廣東臨時省議會成立，公布21歲以上廣東籍人皆有選舉權和被選舉權，議會由120名議員組成，其中同盟會代表20名，軍團協會代表21名，華僑代表12名，師生代表9名，「自治團」代表1名，各地區代表57名。其中女性議員須占10名，開中國婦女參政先河。1913年4月27日，廣東省議會成立，羅曉峰任議長。1918年10月，廣州市市政公所成立。1921年2月廣州正式設市，成立廣州市政廳，孫科任市長。1917年至1922年間，孫中山以及西南各省的國會代表兩次在廣州成立護法軍政府。1925年孫中山逝世，同年7月廣州國民政府成立，7月4日成立廣州市市政府，實行市政委員會制，伍朝樞任市政委員會委員長。1929年，廣州市實行市長制。1949年10月14日中國人民解放軍占領廣州，28日成立廣州市軍事管理委員會，葉劍英任主席。目前廣州市在中華人民共和國政制架構下實行人民代表大會制度，市政府在中共廣州市委的領導下運作，政府駐地越秀區。作為廣東省的省會，廣州市是廣東省人民政府駐地。',\n",
       "  '廣州市軍事管理委員會的第一任主席為？',\n",
       "  '葉劍英'],\n",
       " ['作為近代革命發源地之一，廣州自中華民國代時就是中國社會運動的中心之一。每次全國性的社會運動都有廣州民眾的響應和參與。以廣州為中心的較具規模的社會運動，最早有1925年至1926年在廣州和香港同時舉辦的省港大罷工。廣州市民在1989年更發起活動聲援天安門民主運動，百萬人聚集海珠廣場圍繞廣州解放紀念碑集會。初期廣州媒體以「愛國運動」名義支持。流血事件發生後，民眾暴動，省市政府機關均被衝擊，所有廣州媒體隨即被禁言，亦干擾和封鎖香港電視台。省政府出動軍警鎮壓遊行群眾，大批參與者偷渡至香港、台灣及海外。各企事業單位亦派員審查各部門職工是否有組織或參與集會。1999年的全國性反美活動，有數十萬群眾、學生在市內遊行示威，抗議北約轟炸中國駐南斯拉夫大使館，同時駐廣州美國領事館也受到部分激進示威人士破壞。廣州媒體對此進行全程跟進，但對廣州美國領館破壞情況則完全沒有提及。2005年的全國性反日示威，也有數十萬人在主幹道遊行，不過廣州封鎖消息，大學和中學也禁止學生遊行，否則開除學籍。',\n",
       "  '省港大罷工是除了廣州以外還有哪個地區參與？',\n",
       "  '香港'],\n",
       " ['作為近代革命發源地之一，廣州自中華民國代時就是中國社會運動的中心之一。每次全國性的社會運動都有廣州民眾的響應和參與。以廣州為中心的較具規模的社會運動，最早有1925年至1926年在廣州和香港同時舉辦的省港大罷工。廣州市民在1989年更發起活動聲援天安門民主運動，百萬人聚集海珠廣場圍繞廣州解放紀念碑集會。初期廣州媒體以「愛國運動」名義支持。流血事件發生後，民眾暴動，省市政府機關均被衝擊，所有廣州媒體隨即被禁言，亦干擾和封鎖香港電視台。省政府出動軍警鎮壓遊行群眾，大批參與者偷渡至香港、台灣及海外。各企事業單位亦派員審查各部門職工是否有組織或參與集會。1999年的全國性反美活動，有數十萬群眾、學生在市內遊行示威，抗議北約轟炸中國駐南斯拉夫大使館，同時駐廣州美國領事館也受到部分激進示威人士破壞。廣州媒體對此進行全程跟進，但對廣州美國領館破壞情況則完全沒有提及。2005年的全國性反日示威，也有數十萬人在主幹道遊行，不過廣州封鎖消息，大學和中學也禁止學生遊行，否則開除學籍。',\n",
       "  '廣州媒體聲援天安門民主運動一開始以甚麼名義？',\n",
       "  '愛國運動'],\n",
       " ['作為近代革命發源地之一，廣州自中華民國代時就是中國社會運動的中心之一。每次全國性的社會運動都有廣州民眾的響應和參與。以廣州為中心的較具規模的社會運動，最早有1925年至1926年在廣州和香港同時舉辦的省港大罷工。廣州市民在1989年更發起活動聲援天安門民主運動，百萬人聚集海珠廣場圍繞廣州解放紀念碑集會。初期廣州媒體以「愛國運動」名義支持。流血事件發生後，民眾暴動，省市政府機關均被衝擊，所有廣州媒體隨即被禁言，亦干擾和封鎖香港電視台。省政府出動軍警鎮壓遊行群眾，大批參與者偷渡至香港、台灣及海外。各企事業單位亦派員審查各部門職工是否有組織或參與集會。1999年的全國性反美活動，有數十萬群眾、學生在市內遊行示威，抗議北約轟炸中國駐南斯拉夫大使館，同時駐廣州美國領事館也受到部分激進示威人士破壞。廣州媒體對此進行全程跟進，但對廣州美國領館破壞情況則完全沒有提及。2005年的全國性反日示威，也有數十萬人在主幹道遊行，不過廣州封鎖消息，大學和中學也禁止學生遊行，否則開除學籍。',\n",
       "  '在全國性反日示威的6年前的抗議活動是在抗議甚麼？',\n",
       "  '北約轟炸中國駐南斯拉夫大使館']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.train[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "bEyDq_PC1i82",
    "outputId": "6e6dafc9-e7b0-408f-8b80-337644122cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "1000 batch LOSS 7.8418049812316895\n",
      "1000 batch LOSS 4.971542835235596\n",
      "1000 batch LOSS 4.140658855438232\n",
      "1000 batch LOSS 4.296553134918213\n",
      "1000 batch LOSS 4.236326694488525\n",
      "1000 batch LOSS 4.575427532196045\n",
      "Dev...4298\n",
      "0 0\n",
      "Result: LOSS: 4.744642734527588 AVG: 0.004413621034473181 \n",
      "===Epoches: 0 Loss 32.91886520385742===\n",
      "1000 batch LOSS 3.187732696533203\n",
      "1000 batch LOSS 3.6930763721466064\n",
      "1000 batch LOSS 3.270188331604004\n",
      "1000 batch LOSS 3.3041183948516846\n",
      "1000 batch LOSS 3.5345613956451416\n",
      "1000 batch LOSS 3.6018013954162598\n",
      "Dev...4298\n",
      "0 0\n",
      "Result: LOSS: 4.9558610916137695 AVG: 0.00461010355502367 \n",
      "===Epoches: 1 Loss 22.86084747314453===\n",
      "1000 batch LOSS 3.05202317237854\n",
      "1000 batch LOSS 2.99455189704895\n",
      "1000 batch LOSS 2.9189369678497314\n",
      "1000 batch LOSS 2.905329704284668\n",
      "1000 batch LOSS 2.7291955947875977\n",
      "1000 batch LOSS 2.722001552581787\n",
      "Dev...4298\n",
      "0 0\n",
      "Result: LOSS: 5.332834720611572 AVG: 0.004960776772350073 \n",
      "===Epoches: 2 Loss 19.200843811035156===\n",
      "1000 batch LOSS 2.1989951133728027\n",
      "1000 batch LOSS 2.5633647441864014\n",
      "1000 batch LOSS 2.693988084793091\n",
      "1000 batch LOSS 2.522826910018921\n",
      "1000 batch LOSS 2.904158353805542\n"
     ]
    }
   ],
   "source": [
    "mode = 'train'\n",
    "\n",
    "if mode == 'train': \n",
    "    print('Train')\n",
    "    best_model = train(ds,args)\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')    \n",
    "    modelname = 'bertDRCD'+'_'+(datetime.now()+timedelta(hours=8)).strftime(\"%m%d_%H%M\")+'.pt' \n",
    "    torch.save(best_model, f'saved_models/{modelname}')\n",
    "    print(f'Train end, model name is {modelname}')\n",
    "\n",
    "elif mode == 'test' or mode == 'dev':\n",
    "    modelname = 'bertDRCD_0808_1213.pt'\n",
    "    test_model = bertDRCD(args.model_type).to(args.device)\n",
    "    test_model.load_state_dict(torch.load(f'saved_models/{modelname}'))\n",
    "    test(test_model,ds,args,mode)\n",
    "\n",
    "#22.53021812438965\n",
    "#22.579822540283203\n",
    "# Result: LOSS: 3.2912609577178955 AVG: 0.003765744622796774 \n",
    "# Result: LOSS: 3.526034116744995 AVG: 0.004002308938652277 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7n5HS36ex16"
   },
   "source": [
    "# Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVfxojHYfKJT"
   },
   "outputs": [],
   "source": [
    "class DRCD():\n",
    "    def __init__(self,model_path,model_type,language): \n",
    "        # device\n",
    "        # language define the language model wanna eat\n",
    "        self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "        # model & tokenizer\n",
    "        # model_type = 'hfl/chinese-roberta-wwm-ext'\n",
    "        config = BertConfig.from_pretrained(model_type,output_hidden_states=True)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "        self.model = bertDRCD(model_type).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path)) \n",
    "        self.language = language\n",
    "        # 繁簡轉換\n",
    "        self.c2tw = OpenCC('s2t') # china to tw\n",
    "        self.tw2c = OpenCC('t2s') # tw to china\n",
    "\n",
    "    def process(self,content,question):\n",
    "        \n",
    "        content,question = self.clean_str(content,question)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            token_tensor = self.tokenizer.encode_plus(str(question),str(content),max_length=512,truncation=True,pad_to_max_length=True)\n",
    "            token = torch.tensor(token_tensor['input_ids']).unsqueeze(0).to(self.device)\n",
    "            segment = torch.tensor( token_tensor['token_type_ids']).unsqueeze(0).to(self.device)\n",
    "            mask = torch.tensor( token_tensor['attention_mask'] ).unsqueeze(0).to(self.device)\n",
    "            answer_start,answer_end = self.model(input_ids=token,attention_mask=mask,token_type_ids=segment) \n",
    "      \n",
    "            # get ans\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(token.squeeze())\n",
    "            answer_start = answer_start.argmax(1)\n",
    "            answer_end = answer_end.argmax(1)\n",
    "            #print(answer_start,answer_end)\n",
    "            if answer_start > answer_end :\n",
    "                return '抱歉 這題有點難耶'\n",
    "            \n",
    "            #print(answer_start,answer_end)\n",
    "            answer = ''.join(tokens[answer_start:answer_end+1])\n",
    "            \n",
    "            if self.language == 'tw':\n",
    "                answer = self.c2tw.convert(answer)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def clean_str(self,content,question):\n",
    "        content = content.replace(' ','')\n",
    "        question = question.replace(' ','')\n",
    "        if self.language == 'china':\n",
    "            content = self.tw2c.convert(content)\n",
    "            question = self.tw2c.convert(question)\n",
    "\n",
    "        return content,question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76m8dUy1ew9t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelname = 'bertDRCD_0810_0518.pt'\n",
    "#myfriend = DRCD(f'saved_models/{modelname}','hfl/chinese-roberta-wwm-ext','tw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSFpusMof5hi"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myfriend' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-47b08b083bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2014年世界盃外圍賽，韓國在首輪分組賽以首名出線次輪分組賽，與伊朗、卡達、烏茲別克以及黎巴嫩爭逐兩個直接出線決賽周資格，最後韓國僅以較佳的得失球差壓倒烏茲別克，以小組次名取得2014年世界盃決賽周參賽資格，也是韓國連續八次晉身世界盃決賽周。可惜南韓在決賽周表現不濟，三戰一和兩負小組末席出局。2018年世界盃外圍賽，韓國再次在首輪分組賽以首名出線次輪分組賽，再與伊朗、卡達、烏茲別克同組，同組還有中國及敘利亞。最後韓國以兩分壓倒敘利亞及烏茲別克，再以小組次名取得2018年世界盃決賽周參賽資格，也是韓國連續九次晉身世界盃決賽周。韓國的世界盃成績雖然是亞洲最佳，但在亞洲盃足球賽成績就遠不如世界盃。韓國除了在首兩屆亞洲杯奪冠外，但之後一直與亞洲盃錦標無緣，自1992年至2011年更連續六屆未能打入過亞洲盃決賽。2015年亞洲盃足球賽，韓國以五連勝一球不失的姿態，廿七年來首次打入亞洲盃決賽，對手是東道主澳洲。雖然韓國在分組初賽曾以1-0擊敗澳洲，但這場決賽韓國卻先失一球，最後在下半場補時階段扳平，令比賽進入加時階段，可惜澳洲最後在加時階段攻入致勝一球，最後韓國以1-2敗陣，只得亞軍。'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"哪一個國家負責舉辦2015年亞洲盃足球賽?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyfriend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'myfriend' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#content = '福茂唱片音樂股份有限公司是一家臺灣唱片公司，成立於1961年，由福茂工程創辦人張人鳳及次子張耕宇先生創立。曾為環球音樂旗下子公司之一。至2002年，正式退出環球音樂旗下，成為獨立唱片公司。成立初期，福茂為英國迪卡唱片在台灣的獨家代理公司，1986年成立國語部，開始發展國語唱片市場，庾澄慶為第一個與福茂唱片簽約的台灣歌手。其後福茂一手捧紅歌手包括庾澄慶、邰正宵、辛曉琪、林隆璇和范曉萱等人。1989年福茂唱片引進CD技術，發行的首張CD專輯是庾澄慶的《讓我一次愛個夠》，銷量40萬張，並入選台灣百佳唱片第48位。1992年，迪卡唱片母公司寶麗金唱片向福茂買入60%股權，福茂也同時加入寶麗金唱片旗下；福茂英文名稱因此改為「Decca Records Taiwan Ltd.」並使用迪卡唱片商標。首張以迪卡唱片商標發行的唱片是庾澄慶首張英文專輯《哈林音樂電台》。在此時期，福茂一手捧紅大陸歌手那英和香港歌手周慧敏、蘇永康、王菲等在台灣的國語市場。1999年，寶麗金唱片集團被現時的環球唱片收購，福茂也曾為環球旗下子公司之一。'\n",
    "content = '2014年世界盃外圍賽，韓國在首輪分組賽以首名出線次輪分組賽，與伊朗、卡達、烏茲別克以及黎巴嫩爭逐兩個直接出線決賽周資格，最後韓國僅以較佳的得失球差壓倒烏茲別克，以小組次名取得2014年世界盃決賽周參賽資格，也是韓國連續八次晉身世界盃決賽周。可惜南韓在決賽周表現不濟，三戰一和兩負小組末席出局。2018年世界盃外圍賽，韓國再次在首輪分組賽以首名出線次輪分組賽，再與伊朗、卡達、烏茲別克同組，同組還有中國及敘利亞。最後韓國以兩分壓倒敘利亞及烏茲別克，再以小組次名取得2018年世界盃決賽周參賽資格，也是韓國連續九次晉身世界盃決賽周。韓國的世界盃成績雖然是亞洲最佳，但在亞洲盃足球賽成績就遠不如世界盃。韓國除了在首兩屆亞洲杯奪冠外，但之後一直與亞洲盃錦標無緣，自1992年至2011年更連續六屆未能打入過亞洲盃決賽。2015年亞洲盃足球賽，韓國以五連勝一球不失的姿態，廿七年來首次打入亞洲盃決賽，對手是東道主澳洲。雖然韓國在分組初賽曾以1-0擊敗澳洲，但這場決賽韓國卻先失一球，最後在下半場補時階段扳平，令比賽進入加時階段，可惜澳洲最後在加時階段攻入致勝一球，最後韓國以1-2敗陣，只得亞軍。'\n",
    "question = \"哪一個國家負責舉辦2015年亞洲盃足球賽?\"\n",
    "ans = myfriend.process(content,question)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor q in question:\\n    print(q + ':  '+ myfriend.process(content,q))\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = \\\n",
    "'美國國家情報總監辦公室7日發布新聞稿，分析外國影響美國總統選舉的最新情勢。美方評估，北京認為美國總統川普「無法預測」，希望川普不要贏得連任。川普說，若拜登當總統，則「中國將擁有我們這個國家。」\\\n",
    "川普7日晚間在紐澤西的鄉村俱樂部舉行記者會，被問到此事說，中國大陸、俄羅斯和伊朗都不想看到他贏得選舉，「若拜登當總統，則中國將擁有我們這個國家。」他也暗示，這三個國家可能透過郵寄投票，在美國大選中作弊。\\\n",
    "川普稱自己已聽取這分簡報，但似乎沒搞清楚內容，記者詢問「俄羅斯正在傷害拜登、而中國想看到你輸，你相信這分情報嗎？」\\\n",
    "川普說，可能是真的、可能是假的，實際上他做了很多事，包括醫療物資的儲備系統和軍隊，其中促使北約(NATO)各國提高軍事預算，用來抵抗俄羅斯，「沒人像我對俄羅斯這麼強硬，俄羅斯最不想看到我當選。」\\\n",
    "川普說，中國大陸做夢都想看到拜登打敗他成為總統，「若拜登是總統，則中國會擁有我們這個國家。」\\\n",
    "當記者試圖糾正川普，他接著說，「你沒把報告看清楚吧，報告裡還有個伊朗，伊朗也不想看到我當總統，但我要說，若我贏得連任，我會很快與伊朗、與北韓達成協議；若不是我2016年當選總統，可能美朝之間早已開戰。」\\\n",
    "川普說，若拜登主政下的美國，與中國大陸達成任何協議，則中國大陸會擁有美國；他則可以從中國大陸手中，拿到數百億美元，且美國現在一片欣欣向榮，各種指數表現良好。\\\n",
    "川普說，中國大陸、俄羅斯和伊朗都不想看到他連任，但是最大威脅不是這三個國家，是郵寄投票，包括這些國家甚至北韓等國，都可以在這裡作弊，他會仔細留意這個大問題。'\n",
    "\n",
    "print(len(content))\n",
    "\n",
    "question = ['蒂埃里·亨利是誰','蒂埃里·亨利為法國隊出場幾次','亨利曾在國際比賽中上演過帽子戲法嗎?',\n",
    "            '蒂埃里·亨利是誰在哪年歐洲國家盃外圍賽中曾單場上演大四喜?','亨利在熱身賽中共打進多少球?']\n",
    "\"\"\"\n",
    "for q in question:\n",
    "    print(q + ':  '+ myfriend.process(content,q))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蒂埃里·亨利是誰:  前法國足球運動員\n",
      "蒂埃里·亨利為法國隊出場幾次:  123次\n",
      "亨利曾在國際比賽中上演過帽子戲法嗎?:  從未\n",
      "蒂埃里·亨利是誰在哪年歐洲國家盃外圍賽中曾單場上演大四喜?:  2004年\n",
      "亨利在熱身賽中共打進多少球?:  16球\n"
     ]
    }
   ],
   "source": [
    "content = \\\n",
    "'蒂埃里·亨利是前法國足球運動員。在他職業生涯參加的國際賽事中，他為法國隊出場123次，\\\n",
    "打進51球。他的首個國際比賽進球是在1998年世界盃對陣南非的比賽中。截至2015年10月，他是法國隊的頭號射手\\\n",
    "。2007年10月，他在對陣立陶宛的比賽中打進兩球，打破了米歇爾·普拉蒂尼42球的法國隊進球紀錄。亨利於2010年7月正式退役\\\n",
    "。亨利在2009年10月對奧地利的比賽中打進了個人國家隊第51粒進球，這也是他的最後一粒進球\\\n",
    "。亨利從未在國際比賽中上演過帽子戲法，儘管他曾7次在單場比賽梅開二度。\\\n",
    "他在對陣馬爾他的比賽中進球最多，在2004年歐洲國家盃外圍賽中曾單場上演大四喜。\\\n",
    "亨利一半以上的進球來自於主場比賽，他的51個進球中有31個是在法國本土打進，\\\n",
    "其中有20個在法蘭西體育場。亨利在熱身賽中共打進16球。在2003年國際足總洲際國家盃上\\\n",
    "，亨利打進四球並榮膺最佳射手，他也因此被評為「賽事最傑出球員」。\\\n",
    "亨利在歐洲國家盃外圍賽中打入12球，其中在2004年歐洲杯外圍賽打進6球，他最終排在射手榜第三位。'\n",
    "\n",
    "question = ['蒂埃里·亨利是誰','蒂埃里·亨利為法國隊出場幾次','亨利曾在國際比賽中上演過帽子戲法嗎?',\n",
    "            '蒂埃里·亨利是誰在哪年歐洲國家盃外圍賽中曾單場上演大四喜?','亨利在熱身賽中共打進多少球?']\n",
    "\n",
    "for q in question:\n",
    "    print(q + ':  '+ myfriend.process(content,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfriend = DRCD(f'saved_models/{modelname}','hfl/chinese-roberta-wwm-ext','tw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "罷免韓國瑜第二階段有通過嗎:  2020年4月17日，高雄市選舉委員會公告罷免有效聯署書達到37.7萬，罷韓第二階段通過\n",
      "中華民國選舉史上首位被罷免成功的縣市首長是誰:  韓國瑜\n",
      "罷免有效聯署書達到多少?:  37.7萬\n",
      "韓國瑜在哪天正式解職:  6月12日\n",
      "韓國瑜幾歲被轉入放牛班:  國中三年級\n",
      "韓國瑜每天上課一直看誰小腿:  前排女同學\n",
      "韓國瑜的碩士論文名稱?:  《從中共「對臺統戰」策略看兩航談判》\n",
      "韓國瑜退伍前一年考上什麼:  東吳大學英國語文學系\n",
      "韓國瑜為什麼成績一落千丈:  青春期懵懂，每天上課一直看前排女同學的小腿\n",
      "韓國瑜擔任誰的助理:  馮定國\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "content = '2020年4月17日，高雄市選舉委員會公告罷免有效聯署書達到37.7萬，罷韓第二階段通過\\\n",
    "，韓國瑜成為全國第一位被罷免投票兩次者、也成為首個市長罷免案進入投票階段的市長。2020年6月6日\\\n",
    "，罷免投票通過，6月12日中央選舉委員會公告結果後正式解職\\\n",
    "，韓國瑜成為中華民國選舉史上首位被罷免成功的縣市首長。\\\n",
    "韓國瑜曾自述，他在年少求學階段考試都考第一名，但國中二年級開始因青春期懵懂，\\\n",
    "每天上課一直看前排女同學的小腿，成績一落千丈，國中三年級被轉入放牛班，逃學翹課\\\n",
    "、打撞球、打架也樣樣俱全，最後在父母決定下，18歲時入中華民國陸軍軍官學校專修學生班40期\\\n",
    "，軍銜升至上尉，自稱在軍中受一位國立臺灣大學醫學系畢業的預官轉魔術方塊的啟發，感到自己的不足\\\n",
    "，決定重拾課本。退伍前一年考上東吳大學英國語文學系，\\\n",
    "畢業後又考取國立政治大學東亞研究所和淡江大學國際事務與戰略研究所。\\\n",
    "由於政大免學費還發獎學金故選擇政大東亞所就讀。就讀期間擔任臺北市議員馮定國的助理，\\\n",
    "畢業後獲法學碩士學位。碩士論文名稱為《從中共「對臺統戰」策略看兩航談判》，指導教授為蘇起。'\n",
    "\n",
    "question = ['罷免韓國瑜第二階段有通過嗎','中華民國選舉史上首位被罷免成功的縣市首長是誰'\n",
    "            ,'罷免有效聯署書達到多少?','韓國瑜在哪天正式解職','韓國瑜幾歲被轉入放牛班'\n",
    "            ,'韓國瑜每天上課一直看誰小腿','韓國瑜的碩士論文名稱?',\n",
    "            '韓國瑜退伍前一年考上什麼','韓國瑜為什麼成績一落千丈','韓國瑜擔任誰的助理']\n",
    "\n",
    "for q in question:\n",
    "    print(q + ':  '+ myfriend.process(content,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeXT公司是誰創造的:  賈伯斯\n",
      "NeXT公司是在哪年成立:  1985年\n",
      "誰決議買下NeXT公司:  董事會\n",
      "誰成立了皮克斯:  賈伯斯\n",
      "誰是《玩具總動員》的執行製片人:  賈伯斯\n",
      "皮克斯動畫在哪年被華特迪士尼公司收購:  2006年\n"
     ]
    }
   ],
   "source": [
    "content = '賈伯斯在1970年代末與蘋果公司另一始創人史蒂夫·沃茲尼克及首任投資者邁克·馬庫拉協同其他人設計、\\\n",
    "開發及銷售Apple II系列。在1980年代初，賈伯斯是最早看到全錄帕洛奧圖中心（Xerox PARC）的滑鼠驅動圖形用戶介面的商業潛力，\\\n",
    "並將其應用於Apple Lisa及一年後的麥金塔電腦。1985年，在董事會的鬥爭失勢後，賈伯斯離開蘋果公司及成立了NeXT公司(一間電腦\\\n",
    "平台開發公司，專門從事高等教育及商業市場)在1986年，他收購了盧卡斯影業的電腦繪圖部門，成立了皮克斯（Pixar）。\\\n",
    "他被譽為《玩具總動員》（1995年）的執行製片人。他一直擔任皮克斯動畫的執行長並持有50.1%的股份，\\\n",
    "直到公司在2006年被華特迪士尼公司收購，此項收購使賈伯斯成為迪士尼公司的最大個人股東（有7.4%的股份）及董事會成員。\\\n",
    "在1996年，蘋果公司董事會決議買下NeXT公司，把賈伯斯帶回他參與創立，卻正在垂死邊緣的蘋果公司擔任臨時CEO。\\\n",
    "他在2000年起成為正式CEO，帶領蘋果輝煌的iPod、iPhone、iPad時代的到來。從2003年10月起，賈伯斯與胰腺神經內分泌腫瘤奮戰了8年，\\\n",
    "最終於2011年8月辭任執行長一職，在他第3次病假期間，賈伯斯當選為蘋果公司的董事長。'\n",
    "\n",
    "\n",
    "question = ['NeXT公司是誰創造的','NeXT公司是在哪年成立','誰決議買下NeXT公司','誰成立了皮克斯'\n",
    "            ,'誰是《玩具總動員》的執行製片人','皮克斯動畫在哪年被華特迪士尼公司收購']\n",
    "\n",
    "\n",
    "for q in question:\n",
    "    print(q + ':  '+ myfriend.process(content,q))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "叡揚資訊獲得了甚麼獎項:  抱歉 這題有點難耶\n"
     ]
    }
   ],
   "source": [
    "content = f'叡揚資訊近二十幾年發展商業智慧及資料倉儲系統的建置經驗中，\\\n",
    "提供了以數據分析及 AI 輔助的智慧醫療與智慧零售的解決方案，\\\n",
    "更因此獲得了2019 APICTA Awards 大數據應用銀牌的肯定。\\\n",
    "APICTA 亞太資通訊聯盟大賽，這個舉辦已 18 屆，\\\n",
    "每年超過 200 個各國企業／團隊參賽的競賽，是亞太區最具影響力的資通訊科技競賽，\\\n",
    "更有亞太資通訊科技奧斯卡獎之稱。感謝評審對叡揚大數據應用解決方案肯定，\\\n",
    "對台灣的軟實力更是一劑強心針讓台灣與叡揚在國際的舞台繼續發光發熱。'\n",
    "\n",
    "question = ['APICTA 亞太資通訊聯盟大賽每ㄋㄧㄞˊ']\n",
    "\n",
    "for q in question:\n",
    "    print(q + ':  '+ myfriend.process(content,q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 1, 2, 0, 1, 2]\n",
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2222222222222222, 0.3333333333333333, 0.26666666666666666, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
    "y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
    "precision_recall_fscore_support(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNVM3uYt3pO9lYSGqAyojSw",
   "collapsed_sections": [
    "1Rru4MXtiSQ0"
   ],
   "include_colab_link": true,
   "name": "trainDRCD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
